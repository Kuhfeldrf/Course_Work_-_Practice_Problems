{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook contains two parts. **Part 1, Multiple Linear Regression**, provides you an opportunity to demonstrate your ability to apply course concepts by implementing a training function for multiple linear regression. **Part 2, California Housing Prices**, provides you an opportunity to practice using widely-used ML libraries and an ML workflow to solve a regression problem.\n\n**You do not need to complete Part 1 in order to complete Part 2**. If you get stuck on Part 1, and choose to work on Part 2, be sure that all of your code for Part 1 runs without error. You can comment out your code in Part 1 if necessary.","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Implementing Multiple Linear Regression\n\nGiven a simple MultipleLinearRegressor, and a simple training set of housing data, demonstrate your ability to implement a multiple linear regression model's `fit` function, such that it properly trains its linear model using gradient descent.\n\n## The UnivariateLinearRegressor\n\nLet's first review the UnivariateLinearRegressor, which you should find familiar, and you do not need to modify. Notice that the `fit` method uses a fixed number of iterations, only for simplicity and experimentation.","metadata":{}},{"cell_type":"code","source":"class UnivariateLinearRegressor:\n\n    def __init__(self, w = 0, b = 0, alpha = 0.1):\n        self.w = w\n        self.b = b\n        self.alpha = alpha\n\n    def fit(self, x_train, y_train):\n        for _ in range(0, 500):\n            delta_w = self.alpha * self._d_cost_function_w(x_train, y_train)\n            delta_b = self.alpha * self._d_cost_function_b(x_train, y_train)\n            self.w = self.w - delta_w\n            self.b = self.b - delta_b\n\n    def _d_cost_function_w(self, x_train, y_train):\n        sum = 0\n        for i in range(len(x_train)):\n            sum += (self.predict(x_train[i]) - y_train[i]) * x_train[i]\n        return sum / len(x_train)\n\n    def _d_cost_function_b(self, x_train, y_train):\n        sum = 0\n        for i in range(len(x_train)):\n            sum += (self.predict(x_train[i]) - y_train[i])\n        return sum / len(x_train)\n\n    def predict(self, x):\n        return self.w * x + self.b\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:20:27.071233Z","iopub.execute_input":"2023-05-17T17:20:27.071628Z","iopub.status.idle":"2023-05-17T17:20:27.084650Z","shell.execute_reply.started":"2023-05-17T17:20:27.071595Z","shell.execute_reply":"2023-05-17T17:20:27.083382Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Next, consider the following simple training examples, which you should also find familiar, that represent the square feet and prices of houses.","metadata":{}},{"cell_type":"code","source":"x_train = [1.0, 2.0]\ny_train = [300.0, 500.0]","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:20:30.036907Z","iopub.execute_input":"2023-05-17T17:20:30.038328Z","iopub.status.idle":"2023-05-17T17:20:30.042842Z","shell.execute_reply.started":"2023-05-17T17:20:30.038288Z","shell.execute_reply":"2023-05-17T17:20:30.041843Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"As demonstrated in the related Exploration, we can instantiate, train and make predictions with our UnivariateLinearRegressor as follows. Notice how we first instantiate our UnivariateLinearRegressor with a _single_ weight, and the bias and learning rate.","metadata":{}},{"cell_type":"code","source":"regressor = UnivariateLinearRegressor(0, 0, 0.1)\nregressor.fit(x_train, y_train)\n\nsmall_house_price = regressor.predict(1.0)\nprint(f\"The price of a 1,000 sqft house is {small_house_price}\")\n\nmedium_house_price = regressor.predict(2.0)\nprint(f\"The price of a 2,000 sqft house is {medium_house_price}\")\n\nbig_house_price = regressor.predict(8.0)\nprint(f\"The price of an 8,000 sqft house is {big_house_price}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:20:30.773656Z","iopub.execute_input":"2023-05-17T17:20:30.774047Z","iopub.status.idle":"2023-05-17T17:20:30.783059Z","shell.execute_reply.started":"2023-05-17T17:20:30.774014Z","shell.execute_reply":"2023-05-17T17:20:30.781998Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The price of a 1,000 sqft house is 300.1677608428322\nThe price of a 2,000 sqft house is 499.8963180971484\nThe price of an 8,000 sqft house is 1698.2676616230458\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Observing the results, we can see that the model has made its way toward converging on its line of best fit. However, we are intentionally limiting the amount of training in `fit`, and therefore truncating the training. Again, we are limiting this only for simplicity and experimentation. Try increasing the steps of gradient descent to 500 and re-run the code cells, and notice that the predictions become more accurate.\n\nThis concludes a review of our UnivariateLinearRegressor. Notice that this implementation intentionally handles only one dimension of input. In the example above, this one dimension is the size in square feet of a house.\n","metadata":{}},{"cell_type":"markdown","source":"## The MultipleLinearRegressor\n\nWhile our simple UnivariateLinearRegressor works well for just a single dimension of input, we would like to make predictions based on multiple features, such as square feet, number of bedrooms, the number of floors, and the age of a house.\n\nTo demonstrate your understanding of features, vectors and gradient descent, try completing the implementation of a MultipleLinearRegressor. We begin with the implementation below, which has a complete `predict` method and method stubs for `fit` and the partial derivatives.","metadata":{}},{"cell_type":"code","source":"#Orgiginal functions\nclass MultipleLinearRegressor:\n\n    def __init__(self, w = [], b = 0, alpha = 0.1):\n        self.w = w\n        self.b = b\n        self.alpha = alpha\n\n    def fit(self, x_train, y_train):\n        for _ in range(0, 10):\n            pass\n\n    def _d_cost_function_w(self, x_train, y_train):\n        return 0\n\n    def _d_cost_function_b(self, x_train, y_train):\n        return 0\n\n    def predict(self, x):\n        return self._dot_product(self.w, x) + self.b\n\n    def _dot_product(self, a, b):\n        return sum(pair[0] * pair[1] for pair in zip(a, b))","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:22:00.268466Z","iopub.execute_input":"2023-05-17T17:22:00.268865Z","iopub.status.idle":"2023-05-17T17:22:00.279048Z","shell.execute_reply.started":"2023-05-17T17:22:00.268831Z","shell.execute_reply":"2023-05-17T17:22:00.277894Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"As we shall see in a moment, your goal will be to implement `fit` and `_d_cost_function_w` and `_d_cost_function_b`. For now, let's take a look at the training set and see how our current implementation behaves.\n\nWe'll start with a simple contrived data set with four examples, already split for you. Each training example in `x_train` represents the size, number of bedrooms, number of floors and the age of a house. Each value in `y_train` represents the price of the house in thousands of dollars.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nx_train = np.array([\n    [2104.0, 5.0, 1.0, 45.0],\n    [1416.0, 3.0, 2.0, 40.0],\n    [1534.0, 3.0, 2.0, 30.0],\n    [852.0, 2.0, 1.0, 36.0]])\ny_train = np.array([460.0, 232.0, 315.0, 178.0])","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:23:17.266698Z","iopub.execute_input":"2023-05-17T17:23:17.267093Z","iopub.status.idle":"2023-05-17T17:23:17.274670Z","shell.execute_reply.started":"2023-05-17T17:23:17.267061Z","shell.execute_reply":"2023-05-17T17:23:17.273293Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Notice that `x_train` now contains vectors representing the features of each house, and each vector contains four features. Since we know that our linear regression model will need one weight for each feature, we should instantiate it with a _vector_ of weights, along with a bias and our learning rate.","metadata":{}},{"cell_type":"code","source":"regressor = MultipleLinearRegressor([0, 0, 0, 0], 0, 0.1)\nregressor.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:22:01.850890Z","iopub.execute_input":"2023-05-17T17:22:01.851578Z","iopub.status.idle":"2023-05-17T17:22:01.857459Z","shell.execute_reply.started":"2023-05-17T17:22:01.851534Z","shell.execute_reply":"2023-05-17T17:22:01.856331Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Even though our implementation is incomplete, we can try to make some predictions. Notice that, to make a prediction, we should provide the `predict` method with a vector of features.","metadata":{}},{"cell_type":"code","source":"# 'Test Run' Code Cell, Referred to in \"What to Do\" #2.\n\nfirst_house_price = regressor.predict([2104.0, 5.0, 1.0, 45.0])\nprint(f\"The actual price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is 460 thousand dollars\")\nprint(f\"The predicted price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is {first_house_price} thousand dollars\")\n\nsecond_house_price = regressor.predict([1416.0, 3.0, 2.0, 40.0])\nprint(f\"The actual price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is 232 thousand dollars\")\nprint(f\"The predicted price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is {second_house_price} thousand dollars\")\n\nthird_house_price = regressor.predict([1534.0, 3.0, 2.0, 30.0])\nprint(f\"The actual price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is 315 thousand dollars\")\nprint(f\"The predicated price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is {third_house_price} thousand dollars\")\n\nsmall_house_price = regressor.predict([852.0, 2.0, 1.0, 36.0])\nprint(f\"The actual price of an 852 sqft house with 2 bedrooms, 1 floor, that is 36 years old is 178 thousand dollars\")\nprint(f\"The predicted price of this house is {small_house_price}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:22:02.560630Z","iopub.execute_input":"2023-05-17T17:22:02.561532Z","iopub.status.idle":"2023-05-17T17:22:02.572021Z","shell.execute_reply.started":"2023-05-17T17:22:02.561481Z","shell.execute_reply":"2023-05-17T17:22:02.570916Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"The actual price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is 460 thousand dollars\nThe predicted price of a 2,104 sqft house with 5 bedrooms, 1 floor, that is 45-years old is 0.0 thousand dollars\nThe actual price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is 232 thousand dollars\nThe predicted price of a 1,416 sqft house with 3 bedrooms, 2 floors, that is 40 years old is 0.0 thousand dollars\nThe actual price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is 315 thousand dollars\nThe predicated price of a 1,534 sqft house with 3 bedrooms, 2 floors, that is 30 years old is 0.0 thousand dollars\nThe actual price of an 852 sqft house with 2 bedrooms, 1 floor, that is 36 years old is 178 thousand dollars\nThe predicted price of this house is 0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Notice how, for each example, our MultipleLinearRegressor model is predicting a 0.\n\nOur goal is to complete the implementation of MultipleLinearRegressor, ensuring that we can properly train it.\n","metadata":{}},{"cell_type":"markdown","source":"##Play around with settings","metadata":{}},{"cell_type":"code","source":"class MultipleLinearRegressor:\n\n    def __init__(self, w = [], b = 0, alpha = 0.1):\n        self.w = w\n        self.b = b\n        self.alpha = alpha\n\n    def fit(self, x_train, y_train):\n        #regessor function with optimized number of steps value from 10 to 13.\n        for _ in range(0, 13):\n            delta_w = self.alpha * self._d_cost_function_w(x_train, y_train)\n            delta_b = self.alpha * self._d_cost_function_b(x_train, y_train)\n            self.w = self.w - delta_w\n            self.b = self.b - delta_b\n\n    def _d_cost_function_w(self, x_train, y_train):\n        sum = 0\n        for i in range(len(x_train)):\n            sum += (self.predict(x_train[i]) - y_train[i]) * x_train[i]\n        return sum / len(x_train)\n\n    def _d_cost_function_b(self, x_train, y_train):\n        sum = 0\n        for i in range(len(x_train)):\n            sum += (self.predict(x_train[i]) - y_train[i])\n        return sum / len(x_train)\n\n    def predict(self, x):\n        return self._dot_product(self.w, x) + self.b\n\n    def _dot_product(self, a, b):\n        return sum(pair[0] * pair[1] for pair in zip(a, b))\n    \n#regessor function with optimized alpha value\nregressor = MultipleLinearRegressor([0,0,0,0],0,0.0000001)  \nregressor.fit(x_train, y_train)\n\n# 'Test Run' Code Cell, Referred to in \"What to Do\" #2.\nfirst_house_price = regressor.predict([2104.0, 5.0, 1.0, 45.0])\nprint(f\"[2104.0, 5.0, 1.0, 45.0]= 460\")\nprint(f\"Prediction: {first_house_price} Accuracy:{first_house_price/460*100:.2f}%\")\n\nsecond_house_price = regressor.predict([1416.0, 3.0, 2.0, 40.0])\nprint(f\"[1416.0, 3.0, 2.0, 40.0]= 232\")\nprint(f\"Prediction: {second_house_price} Accuracy:{second_house_price/232*100:.2f}%\")\n\nthird_house_price = regressor.predict([1534.0, 3.0, 2.0, 30.0])\nprint(f\"[1534.0, 3.0, 2.0, 30.0] = 315\")\nprint(f\"Prediction: {third_house_price} Accuracy:{third_house_price/315*100:.2f}%\")\n\nsmall_house_price = regressor.predict([852.0, 2.0, 1.0, 36.0])\nprint(f\"[852.0, 2.0, 1.0, 36.0] = 178\")\nprint(f\"Prediction: {small_house_price} Accuracy:{small_house_price/178*100:.2f}%\")\n\nprint(f\"Average Accuracy: {(((first_house_price/460)+(second_house_price/232)+(third_house_price/315)+(small_house_price/178))/4)*100:.2f}%\")\nprint(f\"Wegihts:{regressor.w}\")\nprint(f\"bais:{regressor.b}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T17:39:01.856008Z","iopub.execute_input":"2023-05-17T17:39:01.856389Z","iopub.status.idle":"2023-05-17T17:39:01.876526Z","shell.execute_reply.started":"2023-05-17T17:39:01.856358Z","shell.execute_reply":"2023-05-17T17:39:01.875238Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"[2104.0, 5.0, 1.0, 45.0]= 460\nPrediction: 414.7003694129901 Accuracy:90.15%\n[1416.0, 3.0, 2.0, 40.0]= 232\nPrediction: 279.13944180273256 Accuracy:120.32%\n[1534.0, 3.0, 2.0, 30.0] = 315\nPrediction: 302.33994318045427 Accuracy:95.98%\n[852.0, 2.0, 1.0, 36.0] = 178\nPrediction: 168.0114471819109 Accuracy:94.39%\nAverage Accuracy: 100.21%\nWegihts:[1.97001949e-01 4.42316980e-04 1.61901405e-04 4.57285511e-03]\nbais:0.00011769819388913334\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## What to Do\n\nImplement `fit`, `_d_cost_function_w` and `_d_cost_function_b`, to represent an appropriate gradient descent algorithm that trains our multiple linear regression model. When complete, you should see the model produce price predictions that begin to approach a \"best fit\" for the simple training data above (note: there are particular reasons why the fit will not be as 'perfect' as our univariate example). Here are some suggestions for completing your implementation.\n\n1. Modify the existing MultipleLinearRegressor class definition above.\n2. Run your code frequently, using _Run All_ and running the code in the \"Test Run\" code cell above.\n2. Draw inspiration from the UnivariateLinearRegressor - the structure of gradient descent remains the same, we just need to handle a vector of weights and features.\n3. Consider replicating the small steps taken in the exploration. Start with `fit`.\n4. Review the Exploration content and familiarize yourself with the expressions for computing the partial derivatives with respect to `w` and `b` when using a _vector_ of weights and features.\n5. Implement just _one_ of the partial derivative functions first, and verify that the prediction output has changed.\n6. For convenience, you can create a new code cell with the class definition, data, instantiation and usage all in one code cell if you wish. But when complete, please be sure that you remove it, and that the MultipleLinearRegressor class definition above is complete.\n\nThe best tip for thinking about this challenge is to become intimately familiar with the expressions for computing the gradients, or partial derivatives, for w and b. Then, try first working out on paper how your implementation of these computations might work, given the vector of weights and features.","metadata":{}},{"cell_type":"markdown","source":"## ðŸ’¡ Conclusion\n**The first thing did was to examine the universal function and replace missing pieces of code in the multiple linear function. Then I tried replacing the cost function methods to independently return 0 which indicated the weight parameters were crucial whereas basis could remain at zero and produce results. Manually decreasing input weights was seen to decrease predicted outputs, however these weights are optimized by the gradient descent function and are not a hyper parameter. The next parameter to tweak was alpha. When alpha was 0.1 the predict values were ~-1e-50 lowering alpha progressively increased the accuracy of the prediction rate. A sweet spot for alpha was found at 0.0000001 which results in an average of 96% accuracy. Then I increased the number of steps from 10 to 13 which increased average accuracy to 100%**\n\n**Both functions use gradient descent to iteratively find weights and basis where the minimal cost function is at its minimum. However with multiple variables you have a more complex model and a vector of weights to adjust compared to a single weight with a univariate function. This resulted in the alpha (learning rate) or step size having to be significantly different in order to find the optimized cost function given the multiple variables. Where as the univariant model has only one weight associated with the learning rate.  In addition the univariant model could simply multiply the single weight by the feature where as the multivariant model would have to use a dot.product to handle to vector of weights and features.**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Predicting California Housing Prices\n\n_Attribution: Special thanks to Dr. Roi Yehoshua_\n\nIn this, the second, part of this notebook, you will construct a guided experiment to analyze the quality of a linear regression model for predicting real housing prices. We'll use a version of the [california housing data](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html) by Kelley and Barry. Take a moment now to [familiarize yourself with the version of this data set provded by sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html), and you can take a look at [a version of this data on Kaggle](https://www.kaggle.com/datasets/camnugent/california-housing-prices). (Note that, the version on Kaggle has an extra column, ocean_proximity, which you should ignore.)\n\nAs you progress through this notebook, complete each code cell, run them, and complete the Knowledge Checks.\n\nWe'll begin by loading the data set.","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Loading the Data Set\n\nFor convenience, we shall rely on the \"california housing set\" provided by scikit-learn. We'll first import a few typical libraries, and fetch the data set.\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\n\nnp.random.seed(0)\ndata = fetch_california_housing(as_frame = True)\nprint(data.DESCR)\n```\n\nTry doing the same in a code cell here.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:20:05.742461Z","iopub.execute_input":"2023-05-17T18:20:05.742885Z","iopub.status.idle":"2023-05-17T18:20:05.748150Z","shell.execute_reply.started":"2023-05-17T18:20:05.742850Z","shell.execute_reply":"2023-05-17T18:20:05.746831Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ndata = pd.read_csv('/kaggle/input/california-housing-prices/housing.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:28:18.351745Z","iopub.execute_input":"2023-05-17T18:28:18.352169Z","iopub.status.idle":"2023-05-17T18:28:18.394495Z","shell.execute_reply.started":"2023-05-17T18:28:18.352132Z","shell.execute_reply":"2023-05-17T18:28:18.393316Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"### ðŸ’¡ Knowledge Check 1\n\nDemonstrate your understanding of the general characteristics of the data set by summarizing it here. (What is this data set, and what does it contain? What are the attributes, what are their types, and what do they mean? What is the target value? Is there missing data? Etc.)\n\n**the data contains information from the 1990 California census. The attributes are:\nlongitude: A measure of how far west a house is; a higher value is farther west\nlatitude: A measure of how far north a house is; a higher value is farther north\nhousingMedianAge: Median age of a house within a block; a lower number is a newer building\ntotalRooms: Total number of rooms within a block\ntotalBedrooms: Total number of bedrooms within a block\npopulation: Total number of people residing within a block\nhouseholds: Total number of households, a group of people residing within a home unit, for a block\nmedianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\nmedianHouseValue: Median house value for households within a block (measured in US Dollars)\noceanProximity: Location of the house w.r.t ocean/sea**\n\n**All are floats except ocean_proximiaty**\n\n**The only attribute with missing values is total_bedrooms which has 207 missing data points. All associated rows with missing data will be dropped prior to train/test split**\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Now that our data set is loaded, let's explore what we have.","metadata":{}},{"cell_type":"code","source":"data.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:37:34.754811Z","iopub.execute_input":"2023-05-17T18:37:34.757289Z","iopub.status.idle":"2023-05-17T18:37:34.783376Z","shell.execute_reply.started":"2023-05-17T18:37:34.757246Z","shell.execute_reply":"2023-05-17T18:37:34.782461Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n5    -122.25     37.85                52.0        919.0           213.0   \n6    -122.25     37.84                52.0       2535.0           489.0   \n7    -122.25     37.84                52.0       3104.0           687.0   \n8    -122.26     37.84                42.0       2555.0           665.0   \n9    -122.25     37.84                52.0       3549.0           707.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n5       413.0       193.0         4.0368            269700.0        NEAR BAY  \n6      1094.0       514.0         3.6591            299200.0        NEAR BAY  \n7      1157.0       647.0         3.1200            241400.0        NEAR BAY  \n8      1206.0       595.0         2.0804            226700.0        NEAR BAY  \n9      1551.0       714.0         3.6912            261100.0        NEAR BAY  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>919.0</td>\n      <td>213.0</td>\n      <td>413.0</td>\n      <td>193.0</td>\n      <td>4.0368</td>\n      <td>269700.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>2535.0</td>\n      <td>489.0</td>\n      <td>1094.0</td>\n      <td>514.0</td>\n      <td>3.6591</td>\n      <td>299200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>3104.0</td>\n      <td>687.0</td>\n      <td>1157.0</td>\n      <td>647.0</td>\n      <td>3.1200</td>\n      <td>241400.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-122.26</td>\n      <td>37.84</td>\n      <td>42.0</td>\n      <td>2555.0</td>\n      <td>665.0</td>\n      <td>1206.0</td>\n      <td>595.0</td>\n      <td>2.0804</td>\n      <td>226700.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>3549.0</td>\n      <td>707.0</td>\n      <td>1551.0</td>\n      <td>714.0</td>\n      <td>3.6912</td>\n      <td>261100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"lets look at the type of varriables. We can see they are all float64, except the irelevent ocean_proximity","metadata":{}},{"cell_type":"code","source":"data.dtypes","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:32:04.069695Z","iopub.execute_input":"2023-05-17T18:32:04.070178Z","iopub.status.idle":"2023-05-17T18:32:04.079691Z","shell.execute_reply.started":"2023-05-17T18:32:04.070137Z","shell.execute_reply":"2023-05-17T18:32:04.078711Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"longitude             float64\nlatitude              float64\nhousing_median_age    float64\ntotal_rooms           float64\ntotal_bedrooms        float64\npopulation            float64\nhouseholds            float64\nmedian_income         float64\nmedian_house_value    float64\nocean_proximity        object\ndtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"Lets look for missing rows and we can see only total_bedrooms has 207 missing data points","metadata":{}},{"cell_type":"code","source":"data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:40:44.358105Z","iopub.execute_input":"2023-05-17T18:40:44.360582Z","iopub.status.idle":"2023-05-17T18:40:44.382010Z","shell.execute_reply.started":"2023-05-17T18:40:44.360533Z","shell.execute_reply":"2023-05-17T18:40:44.380914Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"longitude               0\nlatitude                0\nhousing_median_age      0\ntotal_rooms             0\ntotal_bedrooms        207\npopulation              0\nhouseholds              0\nmedian_income           0\nmedian_house_value      0\nocean_proximity         0\ndtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## Step 2: Exploring the Data Set\n\n","metadata":{}},{"cell_type":"markdown","source":"Let's quickly investigate some examples in the data set. Since `data` is a sklearn Bunch object, we can obtain the pandas DataFrame and investigate its shape, to determine the number of rows and columns, and to inspect the first few rows of data.\n\n```python\nprint(data.frame.shape)\ndata.frame.head()\n```\n\nGo ahead and investigate the first few rows of the data frame.","metadata":{}},{"cell_type":"markdown","source":"lets breifly look at the dataset dimensions, we can see there are 10 atrributes and 20,640 rows","metadata":{}},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:30:55.696888Z","iopub.execute_input":"2023-05-17T18:30:55.697305Z","iopub.status.idle":"2023-05-17T18:30:55.704783Z","shell.execute_reply.started":"2023-05-17T18:30:55.697272Z","shell.execute_reply":"2023-05-17T18:30:55.703580Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"(20640, 10)"},"metadata":{}}]},{"cell_type":"markdown","source":"lets breifly look at the dataset with head function","metadata":{}},{"cell_type":"code","source":"data.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:28:23.487778Z","iopub.execute_input":"2023-05-17T18:28:23.488276Z","iopub.status.idle":"2023-05-17T18:28:23.532962Z","shell.execute_reply.started":"2023-05-17T18:28:23.488232Z","shell.execute_reply":"2023-05-17T18:28:23.532139Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n5    -122.25     37.85                52.0        919.0           213.0   \n6    -122.25     37.84                52.0       2535.0           489.0   \n7    -122.25     37.84                52.0       3104.0           687.0   \n8    -122.26     37.84                42.0       2555.0           665.0   \n9    -122.25     37.84                52.0       3549.0           707.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  \n5       413.0       193.0         4.0368            269700.0        NEAR BAY  \n6      1094.0       514.0         3.6591            299200.0        NEAR BAY  \n7      1157.0       647.0         3.1200            241400.0        NEAR BAY  \n8      1206.0       595.0         2.0804            226700.0        NEAR BAY  \n9      1551.0       714.0         3.6912            261100.0        NEAR BAY  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>919.0</td>\n      <td>213.0</td>\n      <td>413.0</td>\n      <td>193.0</td>\n      <td>4.0368</td>\n      <td>269700.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>2535.0</td>\n      <td>489.0</td>\n      <td>1094.0</td>\n      <td>514.0</td>\n      <td>3.6591</td>\n      <td>299200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>3104.0</td>\n      <td>687.0</td>\n      <td>1157.0</td>\n      <td>647.0</td>\n      <td>3.1200</td>\n      <td>241400.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-122.26</td>\n      <td>37.84</td>\n      <td>42.0</td>\n      <td>2555.0</td>\n      <td>665.0</td>\n      <td>1206.0</td>\n      <td>595.0</td>\n      <td>2.0804</td>\n      <td>226700.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-122.25</td>\n      <td>37.84</td>\n      <td>52.0</td>\n      <td>3549.0</td>\n      <td>707.0</td>\n      <td>1551.0</td>\n      <td>714.0</td>\n      <td>3.6912</td>\n      <td>261100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### ðŸ’¡ Knowledge Check 2\n\nWhat do the `shape` and `head` reveal about this data set?\n\n**head shows us the first number of specified rows, and the names of the associated attributes. Shape tells us the dimensionality(rows and attributes)**\n","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Preparing Training and Test Sets\n\nTo train and test our linear regression model, we will need to split our data set. We'll use the `data` and `target` attributes of the Bunch to retrieve the feature set and target prediction values. Then, we'll reach for the handy `train_test_split` method from sklearn.\n\n```python\nfrom sklearn.model_selection import train_test_split\n\nhousing_attributes, prices = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(housing_attributes, prices, test_size = 0.2)\nX_train.head()\n```\n\nGo ahead and split the data set into training and test sets here.","metadata":{}},{"cell_type":"code","source":"#before splitting the data we need to get ride of Na values\ndata_nona = data.dropna()\nprint(data_nona.shape)\n#to get arround import errors of dataset we are defining our own housing and price attributes\n\n#data features (x-var) removes the median house value and ocean proximity\nhousing_attributes = data_nona.drop(['median_house_value', 'ocean_proximity'], axis=1)\n\n#price data(y)\nprices = data_nona['median_house_value']\n\nfrom sklearn.model_selection import train_test_split\nhousing_attributes, prices\nX_train, X_test, y_train, y_test = train_test_split(housing_attributes, prices, test_size = 0.2)\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:14:38.580359Z","iopub.execute_input":"2023-05-17T19:14:38.580784Z","iopub.status.idle":"2023-05-17T19:14:38.619574Z","shell.execute_reply.started":"2023-05-17T19:14:38.580747Z","shell.execute_reply":"2023-05-17T19:14:38.618508Z"},"trusted":true},"execution_count":109,"outputs":[{"name":"stdout","text":"(20433, 10)\n","output_type":"stream"},{"execution_count":109,"output_type":"execute_result","data":{"text/plain":"       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n7255     -118.15     34.00                32.0       3218.0           739.0   \n19516    -121.01     37.64                52.0        201.0            35.0   \n19640    -120.79     37.53                20.0       1417.0           263.0   \n12633    -121.49     38.49                26.0       4629.0           832.0   \n11750    -121.18     38.78                13.0       3480.0           528.0   \n\n       population  households  median_income  \n7255       2368.0       730.0         3.1406  \n19516        74.0        22.0         1.3036  \n19640       853.0       263.0         3.3083  \n12633      2902.0       816.0         2.7350  \n11750      1432.0       532.0         6.1642  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7255</th>\n      <td>-118.15</td>\n      <td>34.00</td>\n      <td>32.0</td>\n      <td>3218.0</td>\n      <td>739.0</td>\n      <td>2368.0</td>\n      <td>730.0</td>\n      <td>3.1406</td>\n    </tr>\n    <tr>\n      <th>19516</th>\n      <td>-121.01</td>\n      <td>37.64</td>\n      <td>52.0</td>\n      <td>201.0</td>\n      <td>35.0</td>\n      <td>74.0</td>\n      <td>22.0</td>\n      <td>1.3036</td>\n    </tr>\n    <tr>\n      <th>19640</th>\n      <td>-120.79</td>\n      <td>37.53</td>\n      <td>20.0</td>\n      <td>1417.0</td>\n      <td>263.0</td>\n      <td>853.0</td>\n      <td>263.0</td>\n      <td>3.3083</td>\n    </tr>\n    <tr>\n      <th>12633</th>\n      <td>-121.49</td>\n      <td>38.49</td>\n      <td>26.0</td>\n      <td>4629.0</td>\n      <td>832.0</td>\n      <td>2902.0</td>\n      <td>816.0</td>\n      <td>2.7350</td>\n    </tr>\n    <tr>\n      <th>11750</th>\n      <td>-121.18</td>\n      <td>38.78</td>\n      <td>13.0</td>\n      <td>3480.0</td>\n      <td>528.0</td>\n      <td>1432.0</td>\n      <td>532.0</td>\n      <td>6.1642</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#double check price/ y-var split\ny_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-17T18:55:00.550466Z","iopub.execute_input":"2023-05-17T18:55:00.553302Z","iopub.status.idle":"2023-05-17T18:55:00.561477Z","shell.execute_reply.started":"2023-05-17T18:55:00.553245Z","shell.execute_reply":"2023-05-17T18:55:00.560377Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"11217    218000.0\n8038     481300.0\n13031    171000.0\n8464     183300.0\n4550     225000.0\nName: median_house_value, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"### ðŸ’¡ Knowledge Check 3\n\nApproximately how many examples are in the training and test sets?\n","metadata":{}},{"cell_type":"code","source":"print(f\"There are {X_test.shape[0]} examples in the training set and {y_train.shape[0]} in the test set\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:14:47.456730Z","iopub.execute_input":"2023-05-17T19:14:47.457168Z","iopub.status.idle":"2023-05-17T19:14:47.463947Z","shell.execute_reply.started":"2023-05-17T19:14:47.457130Z","shell.execute_reply":"2023-05-17T19:14:47.462784Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"There are 4087 examples in the training set and 16346 in the test set\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Step 4: Pre-Processing and Training\n\nBefore applying our regression model, we would like to standardize the training set. To do this, we'll use the sklearn StandardScaler. Once we standardize the data, we will use it to train a linear regression model. In our case, we will experiment with the scikit-learn [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html), a linear regression model that trains via stochastic gradient descent (SGD). Please be sure to take a look at [the documentation for SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html).\n\nTo demonstrate a new feature in scikit-learn, and to give you some new ideas in your own future work, we will illustrate a small \"machine learning pipeline,\" using the scikit-learn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class.\n\nA Pipeline is handy for \"setting up\" multiple pre-processing steps that will run one after the other. The Pipeline can also end in a training step with a model. This enables us to provide the Pipeline our training data, and with one method call, complete both pre-processing and training in one step.\n\nWe'll import the necessary libraries, create our Pipeline, fill it with a StandardScalar and SGDRegressor, and run the Pipeline.\n\n```python\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor())\n])\n```\n\nTry importing the necessary libraries and building your Pipeline below.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor())\n])","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:24:34.797015Z","iopub.execute_input":"2023-05-17T19:24:34.797449Z","iopub.status.idle":"2023-05-17T19:24:34.803950Z","shell.execute_reply.started":"2023-05-17T19:24:34.797414Z","shell.execute_reply":"2023-05-17T19:24:34.802641Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"With our Pipeline created, we can now invoke the Pipeline's `fit` method, passing it the training data. Behind the scenes, the Pipeline will standardize our training data, and also invoke our SGDRegressor's `fit` method with the transformed training data.\n\n```\npipeline.fit(X_train, y_train)\n```\n\nTry kicking off the Pipeline below.","metadata":{}},{"cell_type":"code","source":"pipeline.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:24:36.926054Z","iopub.execute_input":"2023-05-17T19:24:36.926478Z","iopub.status.idle":"2023-05-17T19:24:36.966738Z","shell.execute_reply.started":"2023-05-17T19:24:36.926440Z","shell.execute_reply":"2023-05-17T19:24:36.965980Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"Pipeline(steps=[('S', StandardScaler()), ('SS', SGDRegressor())])","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;S&#x27;, StandardScaler()), (&#x27;SS&#x27;, SGDRegressor())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;S&#x27;, StandardScaler()), (&#x27;SS&#x27;, SGDRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor()</pre></div></div></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"With our model now trained, let us analyze the results.","metadata":{}},{"cell_type":"markdown","source":"### ðŸ’¡ Knowledge Check 4\n\nInvestigate the parameters passed to the Pipeline initializer. Notice our use of the strings `'scaler'` and `'regressor'`. What purpose do these serve, and are we required to use those specific strings, or can we \"make up\" our own meaningful names for each component of the Pipeline?\n\n**the names \"scaler\" and \"regressor\" in some senses are arbitrary but serve an important purpose for identifying the imported sklearn functions in a rational manner. Yes, we could apply other non-sensical  names, but this would be improper as these specific strings identify the functions and make it easier to reference them by the coder or a future reader of the code.**","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Model Validation\n\nWe have conducted an initial round of training using a data set that may or may not have strong linear tendencies, and we have employed a basic, unconfigured SGDRegressor model to see what baseline quality we can achieve. Let's investigate the \"coefficient of determination,\" R^2, via the model's `score` method. We will invoke this `score` method via the Pipeline, since it has ownership of our SGDRegressor model. We would love to see a value as close to 1.0 as possible.\n\nWe can generate an R^2 score with both the training data and the test data to validate the quality of our model.\n\n```python\ntraining_score = pipeline.score(X_train, y_train)\nprint(f\"Training score: {training_score:.6f}\")\n\ntest_score = pipeline.score(X_test, y_test)\nprint(f\"Test score: {test_score:.6f}\")\n```\n\nGo ahead and generate and print the score based on the training data, and the score based on the test data.","metadata":{}},{"cell_type":"code","source":"training_score = pipeline.score(X_train, y_train)\nprint(f\"Training score:{training_score:.6f}\")\n\ntest_score = pipeline.score(X_test,y_test)\nprint(f\"Test score: {test_score :.6f}\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T19:32:43.453035Z","iopub.execute_input":"2023-05-17T19:32:43.453425Z","iopub.status.idle":"2023-05-17T19:32:43.482320Z","shell.execute_reply.started":"2023-05-17T19:32:43.453396Z","shell.execute_reply":"2023-05-17T19:32:43.480690Z"},"trusted":true},"execution_count":119,"outputs":[{"name":"stdout","text":"Training score:0.634181\nTest score: 0.637807\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ðŸ’¡ Knowledge Check 5\n\nWhat are the scores for the training and test sets? What do they indicate? Are they good? How do you know? (Hint: Have you read the documentation for the `score` method of SGDRegressor?)\n\n**The Training score R^2 = 0.63 and test score R^2 = 0.64. These values indicate there is a positive correlation between the housing attributes and the price of the house. More so the regression prediction does a \"moderate\" job at accurately approximating real data points. an acceptable R^2 values can vary depending on the field but with real world \"messy\" multivariate data such as house prices and attributes a value of 0.63 to me is both logical and good indicating a degree of confidence the code executed correctly on good data.**","metadata":{}},{"cell_type":"markdown","source":"## Step 6: Adjusting the Model (Experiment)\n\nIf we spend time reviewing the documentation of SGDRegressor, we find that the default instantiation uses particular default hyperparameters. Now it's your turn. Based on the concepts in the course and your understanding of linear regression, how might you \"tune\" the SGDRegressor instance in the Pipeline?\n\nTry setting up a new Pipeline as an experiment, and try passing different parameter configurations to SGDRegressor's initializer, and investigate the results. You might set up your experiment like the following. Notice how we have specified a `penalty` of `None` as a demonstrated experiment.\n\n```python\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(penalty = None))\n])\n\npipeline.fit(X_train, y_train)\n\ntraining_score = pipeline.score(X_train, y_train)\nprint(f\"Training score: {training_score:.6f}\")\n\ntest_score = pipeline.score(X_test, y_test)\nprint(f\"Test score: {test_score:.6f}\")\n```\n\nCreate a similar experiment here, and try a few different initialization parameters for SGDRegressor. How might you increase its performance score? (Think about the important concepts of a linear regression model that uses gradient descent. Be sure to try customizing the most important hyperparameters.)","metadata":{}},{"cell_type":"markdown","source":"lets find an optimal model penalty","metadata":{}},{"cell_type":"code","source":"#defines dpenality parameters \npently_paramaters = [\"l2\", \"l1\", \"elasticnet\", None]\n\n#loops over the parameters\nfor para in pently_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(penalty = para))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    para_training_score = pipeline.score(X_train, y_train)\n    para_test_score = pipeline.score(X_test, y_test)\n    print(f\"Penalty: {para}\\nTest score: {para_test_score:.3f}\\nTraining score: {para_training_score:.3f}\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:57:06.272161Z","iopub.execute_input":"2023-05-17T20:57:06.272594Z","iopub.status.idle":"2023-05-17T20:57:06.617599Z","shell.execute_reply.started":"2023-05-17T20:57:06.272562Z","shell.execute_reply":"2023-05-17T20:57:06.616186Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"Penalty: l2\nTest score: 0.634\nTraining score: 0.637\n\nPenalty: l1\nTest score: 0.632\nTraining score: 0.636\n\nPenalty: elasticnet\nTest score: 0.634\nTraining score: 0.637\n\nPenalty: None\nTest score: 0.634\nTraining score: 0.637\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Looks like penality did not have a bit incluence\n\nLets see if we can optimize **alpha** with the same approach using the defualt learing rate 'L2'","metadata":{}},{"cell_type":"code","source":"#defines parameters \nalpha_paramaters = [0,0.1,0.01,0.001,0.0001,0.0001,0.00001,0.000001,0.0000001,0.00000001,0.000000001]\n\n#loops over the parameters\nfor a in alpha_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(alpha = a))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"alpha: {a}\\nTest score: {test_score:.2f}\\nTraining score: {training_score:.2f}\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:32:29.842074Z","iopub.execute_input":"2023-05-17T20:32:29.842446Z","iopub.status.idle":"2023-05-17T20:32:30.656134Z","shell.execute_reply.started":"2023-05-17T20:32:29.842416Z","shell.execute_reply":"2023-05-17T20:32:30.654718Z"},"trusted":true},"execution_count":160,"outputs":[{"name":"stdout","text":"alpha: 0\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 0.1\nTest score: 0.60\nTraining score: 0.60\n\nalpha: 0.01\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 0.001\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 0.0001\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 0.0001\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 1e-05\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 1e-06\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 1e-07\nTest score: 0.63\nTraining score: 0.64\n\nalpha: 1e-08\nTest score: 0.62\nTraining score: 0.64\n\nalpha: 1e-09\nTest score: 0.63\nTraining score: 0.64\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Well it does not look like modifing alpha optimizes that R^2 beyond the defualt of 0.0001. lets examine **max_iter** optimization.","metadata":{}},{"cell_type":"code","source":"#defines parameters \nmi_paramaters = [10,100,1000,10000,100000,1000000,10000000]\n\n#loops over the parameters\nfor mi in mi_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(max_iter = mi))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"Max_iter: {mi}\\nTest score: {test_score:.2f}\\nTraining score: {training_score:.2f}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:32:22.554913Z","iopub.execute_input":"2023-05-17T20:32:22.555319Z","iopub.status.idle":"2023-05-17T20:32:23.056820Z","shell.execute_reply.started":"2023-05-17T20:32:22.555286Z","shell.execute_reply":"2023-05-17T20:32:23.055496Z"},"trusted":true},"execution_count":159,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Max_iter: 10\nTest score: 0.63\nTraining score: 0.64\n\nMax_iter: 100\nTest score: 0.63\nTraining score: 0.64\n\nMax_iter: 1000\nTest score: 0.63\nTraining score: 0.64\n\nMax_iter: 10000\nTest score: 0.63\nTraining score: 0.64\n\nMax_iter: 100000\nTest score: 0.63\nTraining score: 0.64\n\nMax_iter: 1000000\nTest score: 0.63\nTraining score: 0.64\n\nMax_iter: 10000000\nTest score: 0.62\nTraining score: 0.64\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"seems like the defualt settings are just as good at any attempt of hyperparamaterization. Lets optimize the **random_state**","metadata":{}},{"cell_type":"code","source":"#defines parameters \nrs_paramaters = [0,1,2,42]\n\n#loops over the parameters\nfor rs in rs_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(random_state = rs))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"random_state: {rs}\\nTest score: {test_score:.2f}\\nTraining score: {training_score:.2f}\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:32:08.346241Z","iopub.execute_input":"2023-05-17T20:32:08.346625Z","iopub.status.idle":"2023-05-17T20:32:08.590871Z","shell.execute_reply.started":"2023-05-17T20:32:08.346594Z","shell.execute_reply":"2023-05-17T20:32:08.589482Z"},"trusted":true},"execution_count":158,"outputs":[{"name":"stdout","text":"random_state: 0\nTest score: 0.62\nTraining score: 0.64\n\nrandom_state: 1\nTest score: 0.63\nTraining score: 0.64\n\nrandom_state: 2\nTest score: 0.63\nTraining score: 0.64\n\nrandom_state: 42\nTest score: 0.63\nTraining score: 0.64\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Well that didn't work either. Lets investigate manipulating the **train/test split**, using defulat settings for the Regressor","metadata":{}},{"cell_type":"code","source":"#defines parameters \nsplit_paramaters = [.1,.15,.2,.25,.3,.35,.4]\n\n#loops over the parameters\nfor s in split_paramaters:\n    \n    housing_attributes, prices\n    X_train, X_test, y_train, y_test = train_test_split(housing_attributes, prices, test_size = s)\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor())])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"Split: {s}\\nTest score: {test_score:.3f}\\nTraining score: {training_score:.3f}\\nSize of test:{y_test.shape[0]}\\n\")\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:49:41.756029Z","iopub.execute_input":"2023-05-17T20:49:41.756447Z","iopub.status.idle":"2023-05-17T20:49:42.358169Z","shell.execute_reply.started":"2023-05-17T20:49:41.756412Z","shell.execute_reply":"2023-05-17T20:49:42.356671Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stdout","text":"Split: 0.1\nTest score: 0.652\nTraining score: 0.634\nSize of test:2044\n\nSplit: 0.15\nTest score: 0.635\nTraining score: 0.637\nSize of test:3065\n\nSplit: 0.2\nTest score: 0.612\nTraining score: 0.642\nSize of test:4087\n\nSplit: 0.25\nTest score: 0.651\nTraining score: 0.632\nSize of test:5109\n\nSplit: 0.3\nTest score: 0.648\nTraining score: 0.632\nSize of test:6130\n\nSplit: 0.35\nTest score: 0.623\nTraining score: 0.642\nSize of test:7152\n\nSplit: 0.4\nTest score: 0.634\nTraining score: 0.637\nSize of test:8174\n\n","output_type":"stream"}]},{"cell_type":"code","source":"#re-sets to defulat split\nhousing_attributes, prices\nX_train, X_test, y_train, y_test = train_test_split(housing_attributes, prices, test_size = 0.2)\n    \n#defines parameters \nlearning_rate_paramaters = [\"constant\",\"optimal\",\"invscaling\",\"adaptive\"]\n\n#loops over the parameters\nfor lr in learning_rate_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(learning_rate = lr))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"learning_rate: {lr}\\nTest score: {test_score:.3f}\\nTraining score: {training_score:.3f}\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:49:09.690973Z","iopub.execute_input":"2023-05-17T20:49:09.691426Z","iopub.status.idle":"2023-05-17T20:49:10.106991Z","shell.execute_reply.started":"2023-05-17T20:49:09.691391Z","shell.execute_reply":"2023-05-17T20:49:10.105318Z"},"trusted":true},"execution_count":171,"outputs":[{"name":"stdout","text":"learning_rate: constant\nTest score: 0.618\nTraining score: 0.637\n\nlearning_rate: optimal\nTest score: 0.598\nTraining score: 0.614\n\nlearning_rate: invscaling\nTest score: 0.617\nTraining score: 0.641\n\nlearning_rate: adaptive\nTest score: 0.617\nTraining score: 0.641\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"hmmm, that didnt work. Maybe **loss** will work","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"    \n#defines parameters \nloss_paramaters = ['squared_error', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'huber']\n\n#loops over the parameters\nfor l in loss_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(loss = l))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"loss: {lr}\\nTest score: {test_score:.2f}\\nTraining score: {training_score:.2f}\\n\")\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:33:28.396568Z","iopub.execute_input":"2023-05-17T20:33:28.396994Z","iopub.status.idle":"2023-05-17T20:33:31.034282Z","shell.execute_reply.started":"2023-05-17T20:33:28.396953Z","shell.execute_reply":"2023-05-17T20:33:31.028088Z"},"trusted":true},"execution_count":163,"outputs":[{"name":"stdout","text":"loss: adaptive\nTest score: 0.63\nTraining score: 0.64\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"loss: adaptive\nTest score: -3.11\nTraining score: -3.14\n\nloss: adaptive\nTest score: 0.63\nTraining score: 0.64\n\nloss: adaptive\nTest score: -3.19\nTraining score: -3.21\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Lets increase max_iter and see if that worked","metadata":{}},{"cell_type":"code","source":"\"\"\"\n#defines parameters \nloss_paramaters = ['squared_error', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'huber']\n\n#loops over the parameters\nfor l in loss_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(loss = l, max_iter=100000))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"loss: {l}\\nTest score: {test_score:.2f}\\nTraining score: {training_score:.2f}\\n\")\nResults:\nloss: squared_error\nTest score: 0.63\nTraining score: 0.64\n\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(\nloss: epsilon_insensitive\nTest score: -1.11\nTraining score: -1.13\n\nloss: squared_epsilon_insensitive\nTest score: 0.63\nTraining score: 0.64\n\nloss: huber\nTest score: -2.93\nTraining score: -2.96\n\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:40:32.712116Z","iopub.execute_input":"2023-05-17T20:40:32.712502Z","iopub.status.idle":"2023-05-17T20:40:32.722074Z","shell.execute_reply.started":"2023-05-17T20:40:32.712471Z","shell.execute_reply":"2023-05-17T20:40:32.720788Z"},"trusted":true},"execution_count":168,"outputs":[{"execution_count":168,"output_type":"execute_result","data":{"text/plain":"'\\n#defines parameters \\nloss_paramaters = [\\'squared_error\\', \\'epsilon_insensitive\\', \\'squared_epsilon_insensitive\\', \\'huber\\']\\n\\n#loops over the parameters\\nfor l in loss_paramaters:\\n    pipeline = Pipeline([\\n    (\\'scaler\\', StandardScaler()),\\n    (\\'regressor\\', SGDRegressor(loss = l, max_iter=100000))])\\n\\n    #fits model\\n    pipeline.fit(X_train, y_train)\\n    \\n    #calculates training scores\\n    training_score = pipeline.score(X_train, y_train)\\n    test_score = pipeline.score(X_test, y_test)\\n    print(f\"loss: {l}\\nTest score: {test_score:.2f}\\nTraining score: {training_score:.2f}\\n\")\\nResults:\\nloss: squared_error\\nTest score: 0.63\\nTraining score: 0.64\\n\\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\\n  warnings.warn(\\nloss: epsilon_insensitive\\nTest score: -1.11\\nTraining score: -1.13\\n\\nloss: squared_epsilon_insensitive\\nTest score: 0.63\\nTraining score: 0.64\\n\\nloss: huber\\nTest score: -2.93\\nTraining score: -2.96\\n\\n/opt/conda/lib/python3.10/site-packages/sklearn/linear_model/_stochastic_gradient.py:1548: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\\n  warnings.warn(\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"That didn't work either and took forever to run so its commented out.maybe **average** ","metadata":{}},{"cell_type":"code","source":"#defines parameters \navg_paramaters = [0, 1, 10, 100,1000]\n\n#loops over the parameters\nfor avg in avg_paramaters:\n    pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('regressor', SGDRegressor(average = avg))])\n\n    #fits model\n    pipeline.fit(X_train, y_train)\n    \n    #calculates training scores\n    training_score = pipeline.score(X_train, y_train)\n    test_score = pipeline.score(X_test, y_test)\n    print(f\"average: {avg}\\nTest score: {test_score:.2f}\\nTraining score: {training_score:.2f}\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2023-05-17T20:41:33.772468Z","iopub.execute_input":"2023-05-17T20:41:33.773617Z","iopub.status.idle":"2023-05-17T20:41:34.177433Z","shell.execute_reply.started":"2023-05-17T20:41:33.773574Z","shell.execute_reply":"2023-05-17T20:41:34.175891Z"},"trusted":true},"execution_count":170,"outputs":[{"name":"stdout","text":"average: 0\nTest score: 0.63\nTraining score: 0.64\n\naverage: 1\nTest score: 0.63\nTraining score: 0.64\n\naverage: 10\nTest score: 0.63\nTraining score: 0.64\n\naverage: 100\nTest score: 0.63\nTraining score: 0.64\n\naverage: 1000\nTest score: 0.63\nTraining score: 0.64\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### ðŸ’¡ Knowledge Check 6\n\nBased on the concepts in the Explorations regarding linear regression and gradient descent, what is perhaps the single most important hyperparameter for a linear regression model? What SGDRegressor initialization parameter lets you specify the value for this important hyperparameter?\n**An attempt was made to optimize the performance of the model and increase the R^2. This was done through looping over parameters of the Regressor such as  penalty, alpha, max_iter, random_state, learning_rate, loss, and average but no meaningful improvement of R^2 > 0.64 could be achieved. Examining different split/train ratios did little as well to improve R^2 values beyond default settings but a 0.1 split ratio resulted in the best overall training and test R^2 values, but this improvement was trivial.**\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\n\n(Replace this writing prompt with your conclusion.) Summarize what you've seen and done here in Part 2, starting with the domain, problem and data set. Mention three things that were most notable in this process, whether it's related to exploration, preprocessing, configuring, training, or evaluating. If you put in the effort to try to improve the SGDRegressor, describe what you did and what led you to try what you did, and describe the results. Conclude with some statements or questions about the model score, the model being used, and the data set. Make suggestions about what you might do next to either improve the score or conclude with an explanation of whether you would continue to use a linear model.","metadata":{}},{"cell_type":"markdown","source":"**What was accomplished was taking a dataset in this case price of housing in CA and the housing attributes, then addressing the problem of predicting housing price based off these attributes using multivariate regression with gradient descent.**\n\n**Regarding notable things first is just how simple and powerful packages are such as sklearn. The ability to import pre-bult complex functions that are optimized is such a time saver. Secondly, Iâ€™m impressed at how well the default settings of the SGDRegressor performed.  In addition, Iâ€™m surprised the housing attributes did a good job (moderate R^2) at predicting price since it seems like something like housing price, like most of economics is irrational. I guess irrational things can be correlatedâ€¦**\n\n**An attempt was made to optimize the performance of the model and increase the R^2. This was done through looping over parameters of the Regressor such as  penalty, alpha, max_iter, random_state, learning_rate, loss, and average but no meaningful improvement of R^2 > 0.64 could be achieved. Examining different split/train ratios did little as well to improve R^2 values beyond default settings.**\n\n**Iâ€™m a bit surprised the default Regessor model resulted in an optimized R^2, but it does make sense the model is pre-optimized. I wonder how much R^2 would change if the dataset was taken say 1/2020 to now, where there have been crazy swings in economic conditions, likely decreasing correlation. Also, I would be interested to see if the least square method for finding a relationship between housing attributes and price would be just as accurate, and if the simpler approach might result in better data. Or if reducing the features of the dataset could improve correlations. Overall, this model did do a good job at explaining the relationship in the data and I look forward to applying it to other problems**\n","metadata":{}}]}