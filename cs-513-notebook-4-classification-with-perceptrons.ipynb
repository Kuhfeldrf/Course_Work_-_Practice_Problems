{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook contains two parts. **Part 1, Testing Perceptrons**, provides you an opportunity to demonstrate your ability to apply course concepts by implementing a test function for a perceptron. **Part 2, Mine Detection**, provides you an opportunity to practice using widely-used ML libraries and an ML workflow to solve a classification problem.\n\nYou do not need to complete Part 1 in order to complete Part 2. If you get stuck on Part 1, and choose to work on Part 2, be sure that all of your code for Part 1 runs without error. You can comment out your code in Part 1 if necessary.","metadata":{}},{"cell_type":"markdown","source":"# Part 1: Testing Perceptrons\n\nGiven a simple Perceptron classifier, and a test set cancer diagnoses, demonstrate your ability to implement a perceptron's `test` function, such that it returns the accuracy of its predictions for the class labels of samples in a training set.\n\n## The Perceptron Implementation\n\nLet's first introduce the classifier, which you should find familiar, and you do not need to modify. Notice that the `test` method is stubbed.","metadata":{}},{"cell_type":"code","source":"import random\n\nclass Perceptron:\n\n    def __init__(self, alpha = 0.1, max_epochs = 1000):\n        self.alpha = alpha\n        self.max_epochs = max_epochs\n\n    def train(self, training_set):\n        self.weights = self._initial_weights(len(training_set[0]))\n        for i in range(self.max_epochs):\n            correct_classifications = 0\n            for record in training_set:\n                y = record[0]\n                y_hat = self.predict(record[1:])\n                if y == y_hat: correct_classifications += 1\n                self._update_weights(y - y_hat, [1] + record[1:])\n            if correct_classifications / len(training_set) == 1.0:\n                print(f\"Epoch {i} Accuracy: {correct_classifications / len(training_set)}\")\n                break\n\n    def predict(self, features):\n        return self._sign_of(self._dot_product(self.weights, [1] + features))\n\n    def test(self, test_set):\n        pass\n\n    def _update_weights(self, error, features):\n        self.weights[0] += self.alpha * error\n        for i in range(1, len(self.weights)):\n            self.weights[i] += self.alpha * error * features[i]\n\n    def _dot_product(self, w, x):\n        return sum([w * x for w, x in zip(w, x)])\n\n    def _sign_of(self, value):\n        return 1 if value >= 0 else -1\n\n    def _initial_weights(self, length):\n        return [random.uniform(0, 1) for _ in range(length)]","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-27T21:21:45.258512Z","iopub.execute_input":"2023-04-27T21:21:45.258970Z","iopub.status.idle":"2023-04-27T21:21:45.272813Z","shell.execute_reply.started":"2023-04-27T21:21:45.258927Z","shell.execute_reply":"2023-04-27T21:21:45.271583Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## The Data Set\n\nThere is no need for you to manually load the data set. We have provided a subset of the cancer diagnoses data set here, already split into a training set, `diagnoses_training`, and a test set, `diagnoses_test`. Each data set is a simple, two-dimensional Python list, where each sub-list represents the attributes for one diagnosis.\n\nWe have preprocessed the data, such that the **first dimension is the class label**: a `1` indicating malignant, and `-1` indicating benign.\n","metadata":{}},{"cell_type":"code","source":"diagnoses_training = [\n    [1, 17.99, 10.38, 122.8, 1001, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871, 1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193, 25.38, 17.33, 184.6, 2019, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189],\n    [1, 20.57, 17.77, 132.9, 1326, 0.08474, 0.07864, 0.0869, 0.07017, 0.1812, 0.05667, 0.5435, 0.7339, 3.398, 74.08, 0.005225, 0.01308, 0.0186, 0.0134, 0.01389, 0.003532, 24.99, 23.41, 158.8, 1956, 0.1238, 0.1866, 0.2416, 0.186, 0.275, 0.08902],\n    [1, 19.69, 21.25, 130, 1203, 0.1096, 0.1599, 0.1974, 0.1279, 0.2069, 0.05999, 0.7456, 0.7869, 4.585, 94.03, 0.00615, 0.04006, 0.03832, 0.02058, 0.0225, 0.004571, 23.57, 25.53, 152.5, 1709, 0.1444, 0.4245, 0.4504, 0.243, 0.3613, 0.08758],\n    [1, 11.42, 20.38, 77.58, 386.1, 0.1425, 0.2839, 0.2414, 0.1052, 0.2597, 0.09744, 0.4956, 1.156, 3.445, 27.23, 0.00911, 0.07458, 0.05661, 0.01867, 0.05963, 0.009208, 14.91, 26.5, 98.87, 567.7, 0.2098, 0.8663, 0.6869, 0.2575, 0.6638, 0.173],\n    [1, 20.29, 14.34, 135.1, 1297, 0.1003, 0.1328, 0.198, 0.1043, 0.1809, 0.05883, 0.7572, 0.7813, 5.438, 94.44, 0.01149, 0.02461, 0.05688, 0.01885, 0.01756, 0.005115, 22.54, 16.67, 152.2, 1575, 0.1374, 0.205, 0.4, 0.1625, 0.2364, 0.07678],\n    [1, 12.45, 15.7, 82.57, 477.1, 0.1278, 0.17, 0.1578, 0.08089, 0.2087, 0.07613, 0.3345, 0.8902, 2.217, 27.19, 0.00751, 0.03345, 0.03672, 0.01137, 0.02165, 0.005082, 15.47, 23.75, 103.4, 741.6, 0.1791, 0.5249, 0.5355, 0.1741, 0.3985, 0.1244],\n    [1, 18.25, 19.98, 119.6, 1040, 0.09463, 0.109, 0.1127, 0.074, 0.1794, 0.05742, 0.4467, 0.7732, 3.18, 53.91, 0.004314, 0.01382, 0.02254, 0.01039, 0.01369, 0.002179, 22.88, 27.66, 153.2, 1606, 0.1442, 0.2576, 0.3784, 0.1932, 0.3063, 0.08368],\n    [1, 13.71, 20.83, 90.2, 577.9, 0.1189, 0.1645, 0.09366, 0.05985, 0.2196, 0.07451, 0.5835, 1.377, 3.856, 50.96, 0.008805, 0.03029, 0.02488, 0.01448, 0.01486, 0.005412, 17.06, 28.14, 110.6, 897, 0.1654, 0.3682, 0.2678, 0.1556, 0.3196, 0.1151],\n    [1, 13, 21.82, 87.5, 519.8, 0.1273, 0.1932, 0.1859, 0.09353, 0.235, 0.07389, 0.3063, 1.002, 2.406, 24.32, 0.005731, 0.03502, 0.03553, 0.01226, 0.02143, 0.003749, 15.49, 30.73, 106.2, 739.3, 0.1703, 0.5401, 0.539, 0.206, 0.4378, 0.1072],\n    [1, 12.46, 24.04, 83.97, 475.9, 0.1186, 0.2396, 0.2273, 0.08543, 0.203, 0.08243, 0.2976, 1.599, 2.039, 23.94, 0.007149, 0.07217, 0.07743, 0.01432, 0.01789, 0.01008, 15.09, 40.68, 97.65, 711.4, 0.1853, 1.058, 1.105, 0.221, 0.4366, 0.2075],\n    [1, 16.02, 23.24, 102.7, 797.8, 0.08206, 0.06669, 0.03299, 0.03323, 0.1528, 0.05697, 0.3795, 1.187, 2.466, 40.51, 0.004029, 0.009269, 0.01101, 0.007591, 0.0146, 0.003042, 19.19, 33.88, 123.8, 1150, 0.1181, 0.1551, 0.1459, 0.09975, 0.2948, 0.08452],\n    [1, 15.78, 17.89, 103.6, 781, 0.0971, 0.1292, 0.09954, 0.06606, 0.1842, 0.06082, 0.5058, 0.9849, 3.564, 54.16, 0.005771, 0.04061, 0.02791, 0.01282, 0.02008, 0.004144, 20.42, 27.28, 136.5, 1299, 0.1396, 0.5609, 0.3965, 0.181, 0.3792, 0.1048],\n    [1, 19.17, 24.8, 132.4, 1123, 0.0974, 0.2458, 0.2065, 0.1118, 0.2397, 0.078, 0.9555, 3.568, 11.07, 116.2, 0.003139, 0.08297, 0.0889, 0.0409, 0.04484, 0.01284, 20.96, 29.94, 151.7, 1332, 0.1037, 0.3903, 0.3639, 0.1767, 0.3176, 0.1023],\n    [1, 15.85, 23.95, 103.7, 782.7, 0.08401, 0.1002, 0.09938, 0.05364, 0.1847, 0.05338, 0.4033, 1.078, 2.903, 36.58, 0.009769, 0.03126, 0.05051, 0.01992, 0.02981, 0.003002, 16.84, 27.66, 112, 876.5, 0.1131, 0.1924, 0.2322, 0.1119, 0.2809, 0.06287],\n    [1, 13.73, 22.61, 93.6, 578.3, 0.1131, 0.2293, 0.2128, 0.08025, 0.2069, 0.07682, 0.2121, 1.169, 2.061, 19.21, 0.006429, 0.05936, 0.05501, 0.01628, 0.01961, 0.008093, 15.03, 32.01, 108.8, 697.7, 0.1651, 0.7725, 0.6943, 0.2208, 0.3596, 0.1431],\n    [1, 14.54, 27.54, 96.73, 658.8, 0.1139, 0.1595, 0.1639, 0.07364, 0.2303, 0.07077, 0.37, 1.033, 2.879, 32.55, 0.005607, 0.0424, 0.04741, 0.0109, 0.01857, 0.005466, 17.46, 37.13, 124.1, 943.2, 0.1678, 0.6577, 0.7026, 0.1712, 0.4218, 0.1341],\n    [1, 14.68, 20.13, 94.74, 684.5, 0.09867, 0.072, 0.07395, 0.05259, 0.1586, 0.05922, 0.4727, 1.24, 3.195, 45.4, 0.005718, 0.01162, 0.01998, 0.01109, 0.0141, 0.002085, 19.07, 30.88, 123.4, 1138, 0.1464, 0.1871, 0.2914, 0.1609, 0.3029, 0.08216],\n    [1, 16.13, 20.68, 108.1, 798.8, 0.117, 0.2022, 0.1722, 0.1028, 0.2164, 0.07356, 0.5692, 1.073, 3.854, 54.18, 0.007026, 0.02501, 0.03188, 0.01297, 0.01689, 0.004142, 20.96, 31.48, 136.8, 1315, 0.1789, 0.4233, 0.4784, 0.2073, 0.3706, 0.1142],\n    [1, 19.81, 22.15, 130, 1260, 0.09831, 0.1027, 0.1479, 0.09498, 0.1582, 0.05395, 0.7582, 1.017, 5.865, 112.4, 0.006494, 0.01893, 0.03391, 0.01521, 0.01356, 0.001997, 27.32, 30.88, 186.8, 2398, 0.1512, 0.315, 0.5372, 0.2388, 0.2768, 0.07615],\n    [-1, 13.54, 14.36, 87.46, 566.3, 0.09779, 0.08129, 0.06664, 0.04781, 0.1885, 0.05766, 0.2699, 0.7886, 2.058, 23.56, 0.008462, 0.0146, 0.02387, 0.01315, 0.0198, 0.0023, 15.11, 19.26, 99.7, 711.2, 0.144, 0.1773, 0.239, 0.1288, 0.2977, 0.07259],\n    [-1, 13.08, 15.71, 85.63, 520, 0.1075, 0.127, 0.04568, 0.0311, 0.1967, 0.06811, 0.1852, 0.7477, 1.383, 14.67, 0.004097, 0.01898, 0.01698, 0.00649, 0.01678, 0.002425, 14.5, 20.49, 96.09, 630.5, 0.1312, 0.2776, 0.189, 0.07283, 0.3184, 0.08183],\n    [-1, 9.504, 12.44, 60.34, 273.9, 0.1024, 0.06492, 0.02956, 0.02076, 0.1815, 0.06905, 0.2773, 0.9768, 1.909, 15.7, 0.009606, 0.01432, 0.01985, 0.01421, 0.02027, 0.002968, 10.23, 15.66, 65.13, 314.9, 0.1324, 0.1148, 0.08867, 0.06227, 0.245, 0.07773],\n    [1, 15.34, 14.26, 102.5, 704.4, 0.1073, 0.2135, 0.2077, 0.09756, 0.2521, 0.07032, 0.4388, 0.7096, 3.384, 44.91, 0.006789, 0.05328, 0.06446, 0.02252, 0.03672, 0.004394, 18.07, 19.08, 125.1, 980.9, 0.139, 0.5954, 0.6305, 0.2393, 0.4667, 0.09946],\n    [1, 21.16, 23.04, 137.2, 1404, 0.09428, 0.1022, 0.1097, 0.08632, 0.1769, 0.05278, 0.6917, 1.127, 4.303, 93.99, 0.004728, 0.01259, 0.01715, 0.01038, 0.01083, 0.001987, 29.17, 35.59, 188, 2615, 0.1401, 0.26, 0.3155, 0.2009, 0.2822, 0.07526],\n    [1, 16.65, 21.38, 110, 904.6, 0.1121, 0.1457, 0.1525, 0.0917, 0.1995, 0.0633, 0.8068, 0.9017, 5.455, 102.6, 0.006048, 0.01882, 0.02741, 0.0113, 0.01468, 0.002801, 26.46, 31.56, 177, 2215, 0.1805, 0.3578, 0.4695, 0.2095, 0.3613, 0.09564],\n    [1, 17.14, 16.4, 116, 912.7, 0.1186, 0.2276, 0.2229, 0.1401, 0.304, 0.07413, 1.046, 0.976, 7.276, 111.4, 0.008029, 0.03799, 0.03732, 0.02397, 0.02308, 0.007444, 22.25, 21.4, 152.4, 1461, 0.1545, 0.3949, 0.3853, 0.255, 0.4066, 0.1059],\n    [1, 14.58, 21.53, 97.41, 644.8, 0.1054, 0.1868, 0.1425, 0.08783, 0.2252, 0.06924, 0.2545, 0.9832, 2.11, 21.05, 0.004452, 0.03055, 0.02681, 0.01352, 0.01454, 0.003711, 17.62, 33.21, 122.4, 896.9, 0.1525, 0.6643, 0.5539, 0.2701, 0.4264, 0.1275],\n    [1, 18.61, 20.25, 122.1, 1094, 0.0944, 0.1066, 0.149, 0.07731, 0.1697, 0.05699, 0.8529, 1.849, 5.632, 93.54, 0.01075, 0.02722, 0.05081, 0.01911, 0.02293, 0.004217, 21.31, 27.26, 139.9, 1403, 0.1338, 0.2117, 0.3446, 0.149, 0.2341, 0.07421],\n    [1, 15.3, 25.27, 102.4, 732.4, 0.1082, 0.1697, 0.1683, 0.08751, 0.1926, 0.0654, 0.439, 1.012, 3.498, 43.5, 0.005233, 0.03057, 0.03576, 0.01083, 0.01768, 0.002967, 20.27, 36.71, 149.3, 1269, 0.1641, 0.611, 0.6335, 0.2024, 0.4027, 0.09876],\n    [1, 17.57, 15.05, 115, 955.1, 0.09847, 0.1157, 0.09875, 0.07953, 0.1739, 0.06149, 0.6003, 0.8225, 4.655, 61.1, 0.005627, 0.03033, 0.03407, 0.01354, 0.01925, 0.003742, 20.01, 19.52, 134.9, 1227, 0.1255, 0.2812, 0.2489, 0.1456, 0.2756, 0.07919],\n    [1, 18.63, 25.11, 124.8, 1088, 0.1064, 0.1887, 0.2319, 0.1244, 0.2183, 0.06197, 0.8307, 1.466, 5.574, 105, 0.006248, 0.03374, 0.05196, 0.01158, 0.02007, 0.00456, 23.15, 34.01, 160.5, 1670, 0.1491, 0.4257, 0.6133, 0.1848, 0.3444, 0.09782],\n    [1, 11.84, 18.7, 77.93, 440.6, 0.1109, 0.1516, 0.1218, 0.05182, 0.2301, 0.07799, 0.4825, 1.03, 3.475, 41, 0.005551, 0.03414, 0.04205, 0.01044, 0.02273, 0.005667, 16.82, 28.12, 119.4, 888.7, 0.1637, 0.5775, 0.6956, 0.1546, 0.4761, 0.1402],\n    [1, 17.02, 23.98, 112.8, 899.3, 0.1197, 0.1496, 0.2417, 0.1203, 0.2248, 0.06382, 0.6009, 1.398, 3.999, 67.78, 0.008268, 0.03082, 0.05042, 0.01112, 0.02102, 0.003854, 20.88, 32.09, 136.1, 1344, 0.1634, 0.3559, 0.5588, 0.1847, 0.353, 0.08482],\n    [1, 19.27, 26.47, 127.9, 1162, 0.09401, 0.1719, 0.1657, 0.07593, 0.1853, 0.06261, 0.5558, 0.6062, 3.528, 68.17, 0.005015, 0.03318, 0.03497, 0.009643, 0.01543, 0.003896, 24.15, 30.9, 161.4, 1813, 0.1509, 0.659, 0.6091, 0.1785, 0.3672, 0.1123],\n    [1, 16.13, 17.88, 107, 807.2, 0.104, 0.1559, 0.1354, 0.07752, 0.1998, 0.06515, 0.334, 0.6857, 2.183, 35.03, 0.004185, 0.02868, 0.02664, 0.009067, 0.01703, 0.003817, 20.21, 27.26, 132.7, 1261, 0.1446, 0.5804, 0.5274, 0.1864, 0.427, 0.1233],\n    [1, 16.74, 21.59, 110.1, 869.5, 0.0961, 0.1336, 0.1348, 0.06018, 0.1896, 0.05656, 0.4615, 0.9197, 3.008, 45.19, 0.005776, 0.02499, 0.03695, 0.01195, 0.02789, 0.002665, 20.01, 29.02, 133.5, 1229, 0.1563, 0.3835, 0.5409, 0.1813, 0.4863, 0.08633],\n    [1, 14.25, 21.72, 93.63, 633, 0.09823, 0.1098, 0.1319, 0.05598, 0.1885, 0.06125, 0.286, 1.019, 2.657, 24.91, 0.005878, 0.02995, 0.04815, 0.01161, 0.02028, 0.004022, 15.89, 30.36, 116.2, 799.6, 0.1446, 0.4238, 0.5186, 0.1447, 0.3591, 0.1014],\n    [-1, 13.03, 18.42, 82.61, 523.8, 0.08983, 0.03766, 0.02562, 0.02923, 0.1467, 0.05863, 0.1839, 2.342, 1.17, 14.16, 0.004352, 0.004899, 0.01343, 0.01164, 0.02671, 0.001777, 13.3, 22.81, 84.46, 545.9, 0.09701, 0.04619, 0.04833, 0.05013, 0.1987, 0.06169],\n    [1, 14.99, 25.2, 95.54, 698.8, 0.09387, 0.05131, 0.02398, 0.02899, 0.1565, 0.05504, 1.214, 2.188, 8.077, 106, 0.006883, 0.01094, 0.01818, 0.01917, 0.007882, 0.001754, 14.99, 25.2, 95.54, 698.8, 0.09387, 0.05131, 0.02398, 0.02899, 0.1565, 0.05504],\n    [1, 13.48, 20.82, 88.4, 559.2, 0.1016, 0.1255, 0.1063, 0.05439, 0.172, 0.06419, 0.213, 0.5914, 1.545, 18.52, 0.005367, 0.02239, 0.03049, 0.01262, 0.01377, 0.003187, 15.53, 26.02, 107.3, 740.4, 0.161, 0.4225, 0.503, 0.2258, 0.2807, 0.1071],\n    [1, 13.44, 21.58, 86.18, 563, 0.08162, 0.06031, 0.0311, 0.02031, 0.1784, 0.05587, 0.2385, 0.8265, 1.572, 20.53, 0.00328, 0.01102, 0.0139, 0.006881, 0.0138, 0.001286, 15.93, 30.25, 102.5, 787.9, 0.1094, 0.2043, 0.2085, 0.1112, 0.2994, 0.07146],\n    [1, 10.95, 21.35, 71.9, 371.1, 0.1227, 0.1218, 0.1044, 0.05669, 0.1895, 0.0687, 0.2366, 1.428, 1.822, 16.97, 0.008064, 0.01764, 0.02595, 0.01037, 0.01357, 0.00304, 12.84, 35.34, 87.22, 514, 0.1909, 0.2698, 0.4023, 0.1424, 0.2964, 0.09606],\n    [1, 19.07, 24.81, 128.3, 1104, 0.09081, 0.219, 0.2107, 0.09961, 0.231, 0.06343, 0.9811, 1.666, 8.83, 104.9, 0.006548, 0.1006, 0.09723, 0.02638, 0.05333, 0.007646, 24.09, 33.17, 177.4, 1651, 0.1247, 0.7444, 0.7242, 0.2493, 0.467, 0.1038],\n    [1, 13.28, 20.28, 87.32, 545.2, 0.1041, 0.1436, 0.09847, 0.06158, 0.1974, 0.06782, 0.3704, 0.8249, 2.427, 31.33, 0.005072, 0.02147, 0.02185, 0.00956, 0.01719, 0.003317, 17.38, 28, 113.1, 907.2, 0.153, 0.3724, 0.3664, 0.1492, 0.3739, 0.1027],\n    [1, 13.17, 21.81, 85.42, 531.5, 0.09714, 0.1047, 0.08259, 0.05252, 0.1746, 0.06177, 0.1938, 0.6123, 1.334, 14.49, 0.00335, 0.01384, 0.01452, 0.006853, 0.01113, 0.00172, 16.23, 29.89, 105.5, 740.7, 0.1503, 0.3904, 0.3728, 0.1607, 0.3693, 0.09618],\n    [1, 18.65, 17.6, 123.7, 1076, 0.1099, 0.1686, 0.1974, 0.1009, 0.1907, 0.06049, 0.6289, 0.6633, 4.293, 71.56, 0.006294, 0.03994, 0.05554, 0.01695, 0.02428, 0.003535, 22.82, 21.32, 150.6, 1567, 0.1679, 0.509, 0.7345, 0.2378, 0.3799, 0.09185],\n    [-1, 8.196, 16.84, 51.71, 201.9, 0.086, 0.05943, 0.01588, 0.005917, 0.1769, 0.06503, 0.1563, 0.9567, 1.094, 8.205, 0.008968, 0.01646, 0.01588, 0.005917, 0.02574, 0.002582, 8.964, 21.96, 57.26, 242.2, 0.1297, 0.1357, 0.0688, 0.02564, 0.3105, 0.07409]\n]\n\ndiagnoses_test = [\n    [-1, 8.95, 15.76, 58.74, 245.2, 0.09462, 0.1243, 0.09263, 0.02308, 0.1305, 0.07163, 0.3132, 0.9789, 3.28, 16.94, 0.01835, 0.0676, 0.09263, 0.02308, 0.02384, 0.005601, 9.414, 17.07, 63.34, 270, 0.1179, 0.1879, 0.1544, 0.03846, 0.1652, 0.07722],\n    [1, 15.22, 30.62, 103.4, 716.9, 0.1048, 0.2087, 0.255, 0.09429, 0.2128, 0.07152, 0.2602, 1.205, 2.362, 22.65, 0.004625, 0.04844, 0.07359, 0.01608, 0.02137, 0.006142, 17.52, 42.79, 128.7, 915, 0.1417, 0.7917, 1.17, 0.2356, 0.4089, 0.1409],\n    [-1, 11.34, 21.26, 72.48, 396.5, 0.08759, 0.06575, 0.05133, 0.01899, 0.1487, 0.06529, 0.2344, 0.9861, 1.597, 16.41, 0.009113, 0.01557, 0.02443, 0.006435, 0.01568, 0.002477, 13.01, 29.15, 83.99, 518.1, 0.1699, 0.2196, 0.312, 0.08278, 0.2829, 0.08832],\n    [1, 20.92, 25.09, 143, 1347, 0.1099, 0.2236, 0.3174, 0.1474, 0.2149, 0.06879, 0.9622, 1.026, 8.758, 118.8, 0.006399, 0.0431, 0.07845, 0.02624, 0.02057, 0.006213, 24.29, 29.41, 179.1, 1819, 0.1407, 0.4186, 0.6599, 0.2542, 0.2929, 0.09873],\n    [-1, 12.36, 18.54, 79.01, 466.7, 0.08477, 0.06815, 0.02643, 0.01921, 0.1602, 0.06066, 0.1199, 0.8944, 0.8484, 9.227, 0.003457, 0.01047, 0.01167, 0.005558, 0.01251, 0.001356, 13.29, 27.49, 85.56, 544.1, 0.1184, 0.1963, 0.1937, 0.08442, 0.2983, 0.07185],\n    [1, 21.56, 22.39, 142, 1479, 0.111, 0.1159, 0.2439, 0.1389, 0.1726, 0.05623, 1.176, 1.256, 7.673, 158.7, 0.0103, 0.02891, 0.05198, 0.02454, 0.01114, 0.004239, 25.45, 26.4, 166.1, 2027, 0.141, 0.2113, 0.4107, 0.2216, 0.206, 0.07115],\n    [-1, 9.777, 16.99, 62.5, 290.2, 0.1037, 0.08404, 0.04334, 0.01778, 0.1584, 0.07065, 0.403, 1.424, 2.747, 22.87, 0.01385, 0.02932, 0.02722, 0.01023, 0.03281, 0.004638, 11.05, 21.47, 71.68, 367, 0.1467, 0.1765, 0.13, 0.05334, 0.2533, 0.08468],\n    [1, 20.13, 28.25, 131.2, 1261, 0.0978, 0.1034, 0.144, 0.09791, 0.1752, 0.05533, 0.7655, 2.463, 5.203, 99.04, 0.005769, 0.02423, 0.0395, 0.01678, 0.01898, 0.002498, 23.69, 38.25, 155, 1731, 0.1166, 0.1922, 0.3215, 0.1628, 0.2572, 0.06637],\n    [-1, 12.63, 20.76, 82.15, 480.4, 0.09933, 0.1209, 0.1065, 0.06021, 0.1735, 0.0707, 0.3424, 1.803, 2.711, 20.48, 0.01291, 0.04042, 0.05101, 0.02295, 0.02144, 0.005891, 13.33, 25.47, 89, 527.4, 0.1287, 0.225, 0.2216, 0.1105, 0.2226, 0.08486],\n    [1, 16.6, 28.08, 108.3, 858.1, 0.08455, 0.1023, 0.09251, 0.05302, 0.159, 0.05648, 0.4564, 1.075, 3.425, 48.55, 0.005903, 0.03731, 0.0473, 0.01557, 0.01318, 0.003892, 18.98, 34.12, 126.7, 1124, 0.1139, 0.3094, 0.3403, 0.1418, 0.2218, 0.0782],\n    [-1, 14.26, 19.65, 97.83, 629.9, 0.07837, 0.2233, 0.3003, 0.07798, 0.1704, 0.07769, 0.3628, 1.49, 3.399, 29.25, 0.005298, 0.07446, 0.1435, 0.02292, 0.02566, 0.01298, 15.3, 23.73, 107, 709, 0.08949, 0.4193, 0, 0.1503, 0.07247, 0.2438, 0.08541],\n    [1, 20.6, 29.33, 140.1, 1265, 0.1178, 0.277, 0.3514, 0.152, 0.2397, 0.07016, 0.726, 1.595, 5.772, 86.22, 0.006522, 0.06158, 0.07117, 0.01664, 0.02324, 0.006185, 25.74, 39.42, 184.6, 1821, 0.165, 0.8681, 0.9387, 0.265, 0.4087, 0.124]\n]\n","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-04-27T21:21:45.392723Z","iopub.execute_input":"2023-04-27T21:21:45.393422Z","iopub.status.idle":"2023-04-27T21:21:45.477375Z","shell.execute_reply.started":"2023-04-27T21:21:45.393359Z","shell.execute_reply":"2023-04-27T21:21:45.476181Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"## What to Do\n \nDemonstrate your understanding and ability to have synthesized course concepts by implementing a `test` function for the perceptron. Your goal is to implement the `test` function, stubbed for you below, in the subclass `TestablePerceptron`. The `test` function should return the accuracy rate of the perceptron's prediction, given `diagnoses_test`. In the end, your work should reflect the principles seen thus far in the course.\n\nPlease be sure to demonstrate:\n\n1. Your implementation, as code, in the subclass below.\n2. Using your test function, which is already done for you, following the class definition.\n\n## Tips\n\n- Be sure that you have spent time with the Exploration materials in this course.\n- Ask questions on the course forum if you get stuck (describe what you are trying to do, and errors that you encounter)\n- Keep it simple. This is quite straightforward.\n- Be sure to run the code cell containing your TestablePerceptron class and the code cell that invokes the `test` method, or use the *>> Run All* button.","metadata":{}},{"cell_type":"markdown","source":"# Begin Your Implementation Here\nIn the code cell below, implement the test function by replacing pass with your code. It should accept a training set as input, and return a number representing the accuracy of the perceptron based on predictions made with the test set.","metadata":{}},{"cell_type":"markdown","source":"from exploration of material\n> Our Perceptron also does not include an implementation of its test method, and we will not illustrate that here. Such a test method should:\n\n> Iterate over every sample in the test set\n\n> Make a prediction\n\n> Keep track of the accuracy\n\n> Report the result\n\n> Notice that testing a perceptron is not training a perceptron, and that testing involves no training epochs, and no adjustment to a perceptron's weights.","metadata":{}},{"cell_type":"code","source":"class TestablePerceptron(Perceptron):\n    \n    def test(self, test_set):\n        \n        #creates varriable to store accurate predictions\n        acc_pred = 0\n    \n        #Iterate over every sample in the test set and extracts dep. and ind. vars.\n        for data_line in test_set:\n            y = data_line[0]\n            features = data_line[1:0]\n        \n            #Make a prediction and stores the result if correct\n            y_hat = self.predict(features)\n            if y == y_hat:\n                acc_pred += 1\n    \n        #Report and return the results\n        accuracy = acc_pred / len(test_set)\n        return accuracy\n\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:45.480252Z","iopub.execute_input":"2023-04-27T21:21:45.480922Z","iopub.status.idle":"2023-04-27T21:21:45.494281Z","shell.execute_reply.started":"2023-04-27T21:21:45.480852Z","shell.execute_reply":"2023-04-27T21:21:45.493062Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Your TestablePerceptron should result in the following code, which trains and tests a TestablePerceptron, running without error. You do not need to modify the code below, but once your `test` method above is complete, you should see the accuracy of the test printed, instead of `None`.","metadata":{}},{"cell_type":"code","source":"perceptron = TestablePerceptron(alpha = 0.1, max_epochs = 10000)\nperceptron.train(diagnoses_training)\nprint(perceptron.test(diagnoses_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:45.496024Z","iopub.execute_input":"2023-04-27T21:21:45.496361Z","iopub.status.idle":"2023-04-27T21:21:48.781817Z","shell.execute_reply.started":"2023-04-27T21:21:45.496332Z","shell.execute_reply":"2023-04-27T21:21:48.780469Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch 4982 Accuracy: 1.0\n0.5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Conclusion of Part 1\n\nWrite a few sentences here that describes what your `test` function does in \"plain English.\" Try expressing this accurately and authentically *without* giving us a line-by-line summary of what your function does (we can read the code for that). Tell us what the test error is. Then, describe how many learning epochs it took for the perceptron to achive 100% accuracy during *training*, and explain why that number changes each time you run the code cell above.\n\n**The test function asses the accuracy of the method’s ability to correctly determine if a cell is malignant or benign given a specific set of data. It uses numerical descriptions of each cell as the data input in order to make a prediction of malignant vs benign as the output. What the test function does is evaluate this prediction to see what percentage of correct predictions were made out of the entire set of data used to test the method/model.**\n\n**The initial weights of the perceptron and generated randomly so there will be slight variations in the number of epochs required for convergence to 100% accuracy. In this case 5694 epochs were required.**\n\n","metadata":{}},{"cell_type":"markdown","source":"# Part 2: Mine Detection\n\nIn this, the second, part of this notebook, you will build a classifier that can predict whether or not a sonar signature is from a mine or a rock. We'll use a version of the [sonar data set](https://www.openml.org/search?type=data&sort=runs&id=40&status=active) by Gorman and Sejnowski. Take a moment now to [familiarize yourself with the subject matter of this data set](https://datahub.io/machine-learning/sonar%23resource-sonar), and look at the details of the version of this data set, [Mines vs Rocks, hosted on Kaggle](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks).\n\nUnlike previous notebooks, where we provide code for each step of the ML process, this notebook expects each student to implement the ML workflow steps. We will get you started by providing the first step, loading the data, and providing some landmarks below. Your process should demonstrate:\n\n1. Loading the data\n2. Exploring the data\n3. Preprocessing the data\n4. Preparing the training and test sets\n5. Creating and configuring a sklearn.linear_model.Perceptron\n6. Training the perceptron\n7. Testing the perceptron\n8. Demonstrating making predictions\n9. Evaluate (and Improve) the results\n\nCan you train a classifier that can predict whether a sonar signature is from a mine or a rock? \"Three trained human subjects were each tested on 100 signals, chosen at random from the set of 208 returns used to create this data set. Their responses ranged between 88% and 97% correct.\" Can your classifier outperform the human subjects?\n\n","metadata":{}},{"cell_type":"markdown","source":"## Step 1: Load the Data\n\nThe notebook comes pre-bundled with the [Mines vs Rocks data set](https://www.kaggle.com/datasets/mattcarter865/mines-vs-rocks). Our first step is to create a pandas DataFrame from the CSV file. Note that the CSV file has no header row. Loading the CSV file into a DataFrame will make it easy for us to explore the data, preprocess it, and split it into training and test sets.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsonar_csv_path = \"../input/mines-vs-rocks/sonar.all-data.csv\"\nsonar_data = pd.read_csv(sonar_csv_path, header=None)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:48.783472Z","iopub.execute_input":"2023-04-27T21:21:48.784467Z","iopub.status.idle":"2023-04-27T21:21:48.807765Z","shell.execute_reply.started":"2023-04-27T21:21:48.784412Z","shell.execute_reply":"2023-04-27T21:21:48.806623Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"We now have a pandas DataFrame encapsulating the sonar data, and can proceed with our data exploration.","metadata":{}},{"cell_type":"markdown","source":"## Step 2: Explore the Data\n\n**to explore the data we will use the head, description min and max, and shape functions as seen below:**\n","metadata":{}},{"cell_type":"markdown","source":"Lets start out by looking at the first few lines of data","metadata":{}},{"cell_type":"code","source":"sonar_data.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:48.810711Z","iopub.execute_input":"2023-04-27T21:21:48.811389Z","iopub.status.idle":"2023-04-27T21:21:48.846175Z","shell.execute_reply.started":"2023-04-27T21:21:48.811348Z","shell.execute_reply":"2023-04-27T21:21:48.844812Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"       0       1       2       3       4       5       6       7       8   \\\n0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n5  0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n6  0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n7  0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n8  0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n9  0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n\n       9   ...      51      52      53      54      55      56      57  \\\n0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n5  0.3039  ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027   \n6  0.3513  ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143   \n7  0.2838  ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047   \n8  0.1487  ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093   \n9  0.0251  ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035   \n\n       58      59  60  \n0  0.0090  0.0032   R  \n1  0.0052  0.0044   R  \n2  0.0095  0.0078   R  \n3  0.0040  0.0117   R  \n4  0.0107  0.0094   R  \n5  0.0051  0.0062   R  \n6  0.0036  0.0103   R  \n7  0.0048  0.0053   R  \n8  0.0059  0.0022   R  \n9  0.0056  0.0040   R  \n\n[10 rows x 61 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n      <th>60</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0200</td>\n      <td>0.0371</td>\n      <td>0.0428</td>\n      <td>0.0207</td>\n      <td>0.0954</td>\n      <td>0.0986</td>\n      <td>0.1539</td>\n      <td>0.1601</td>\n      <td>0.3109</td>\n      <td>0.2111</td>\n      <td>...</td>\n      <td>0.0027</td>\n      <td>0.0065</td>\n      <td>0.0159</td>\n      <td>0.0072</td>\n      <td>0.0167</td>\n      <td>0.0180</td>\n      <td>0.0084</td>\n      <td>0.0090</td>\n      <td>0.0032</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0453</td>\n      <td>0.0523</td>\n      <td>0.0843</td>\n      <td>0.0689</td>\n      <td>0.1183</td>\n      <td>0.2583</td>\n      <td>0.2156</td>\n      <td>0.3481</td>\n      <td>0.3337</td>\n      <td>0.2872</td>\n      <td>...</td>\n      <td>0.0084</td>\n      <td>0.0089</td>\n      <td>0.0048</td>\n      <td>0.0094</td>\n      <td>0.0191</td>\n      <td>0.0140</td>\n      <td>0.0049</td>\n      <td>0.0052</td>\n      <td>0.0044</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0262</td>\n      <td>0.0582</td>\n      <td>0.1099</td>\n      <td>0.1083</td>\n      <td>0.0974</td>\n      <td>0.2280</td>\n      <td>0.2431</td>\n      <td>0.3771</td>\n      <td>0.5598</td>\n      <td>0.6194</td>\n      <td>...</td>\n      <td>0.0232</td>\n      <td>0.0166</td>\n      <td>0.0095</td>\n      <td>0.0180</td>\n      <td>0.0244</td>\n      <td>0.0316</td>\n      <td>0.0164</td>\n      <td>0.0095</td>\n      <td>0.0078</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0100</td>\n      <td>0.0171</td>\n      <td>0.0623</td>\n      <td>0.0205</td>\n      <td>0.0205</td>\n      <td>0.0368</td>\n      <td>0.1098</td>\n      <td>0.1276</td>\n      <td>0.0598</td>\n      <td>0.1264</td>\n      <td>...</td>\n      <td>0.0121</td>\n      <td>0.0036</td>\n      <td>0.0150</td>\n      <td>0.0085</td>\n      <td>0.0073</td>\n      <td>0.0050</td>\n      <td>0.0044</td>\n      <td>0.0040</td>\n      <td>0.0117</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0762</td>\n      <td>0.0666</td>\n      <td>0.0481</td>\n      <td>0.0394</td>\n      <td>0.0590</td>\n      <td>0.0649</td>\n      <td>0.1209</td>\n      <td>0.2467</td>\n      <td>0.3564</td>\n      <td>0.4459</td>\n      <td>...</td>\n      <td>0.0031</td>\n      <td>0.0054</td>\n      <td>0.0105</td>\n      <td>0.0110</td>\n      <td>0.0015</td>\n      <td>0.0072</td>\n      <td>0.0048</td>\n      <td>0.0107</td>\n      <td>0.0094</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0286</td>\n      <td>0.0453</td>\n      <td>0.0277</td>\n      <td>0.0174</td>\n      <td>0.0384</td>\n      <td>0.0990</td>\n      <td>0.1201</td>\n      <td>0.1833</td>\n      <td>0.2105</td>\n      <td>0.3039</td>\n      <td>...</td>\n      <td>0.0045</td>\n      <td>0.0014</td>\n      <td>0.0038</td>\n      <td>0.0013</td>\n      <td>0.0089</td>\n      <td>0.0057</td>\n      <td>0.0027</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0317</td>\n      <td>0.0956</td>\n      <td>0.1321</td>\n      <td>0.1408</td>\n      <td>0.1674</td>\n      <td>0.1710</td>\n      <td>0.0731</td>\n      <td>0.1401</td>\n      <td>0.2083</td>\n      <td>0.3513</td>\n      <td>...</td>\n      <td>0.0201</td>\n      <td>0.0248</td>\n      <td>0.0131</td>\n      <td>0.0070</td>\n      <td>0.0138</td>\n      <td>0.0092</td>\n      <td>0.0143</td>\n      <td>0.0036</td>\n      <td>0.0103</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0519</td>\n      <td>0.0548</td>\n      <td>0.0842</td>\n      <td>0.0319</td>\n      <td>0.1158</td>\n      <td>0.0922</td>\n      <td>0.1027</td>\n      <td>0.0613</td>\n      <td>0.1465</td>\n      <td>0.2838</td>\n      <td>...</td>\n      <td>0.0081</td>\n      <td>0.0120</td>\n      <td>0.0045</td>\n      <td>0.0121</td>\n      <td>0.0097</td>\n      <td>0.0085</td>\n      <td>0.0047</td>\n      <td>0.0048</td>\n      <td>0.0053</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0223</td>\n      <td>0.0375</td>\n      <td>0.0484</td>\n      <td>0.0475</td>\n      <td>0.0647</td>\n      <td>0.0591</td>\n      <td>0.0753</td>\n      <td>0.0098</td>\n      <td>0.0684</td>\n      <td>0.1487</td>\n      <td>...</td>\n      <td>0.0145</td>\n      <td>0.0128</td>\n      <td>0.0145</td>\n      <td>0.0058</td>\n      <td>0.0049</td>\n      <td>0.0065</td>\n      <td>0.0093</td>\n      <td>0.0059</td>\n      <td>0.0022</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0164</td>\n      <td>0.0173</td>\n      <td>0.0347</td>\n      <td>0.0070</td>\n      <td>0.0187</td>\n      <td>0.0671</td>\n      <td>0.1056</td>\n      <td>0.0697</td>\n      <td>0.0962</td>\n      <td>0.0251</td>\n      <td>...</td>\n      <td>0.0090</td>\n      <td>0.0223</td>\n      <td>0.0179</td>\n      <td>0.0084</td>\n      <td>0.0068</td>\n      <td>0.0032</td>\n      <td>0.0035</td>\n      <td>0.0056</td>\n      <td>0.0040</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 61 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We can explore the data in a similar maner with the describe function and get more summary statistics","metadata":{}},{"cell_type":"code","source":"sonar_data.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:48.848961Z","iopub.execute_input":"2023-04-27T21:21:48.849664Z","iopub.status.idle":"2023-04-27T21:21:49.011043Z","shell.execute_reply.started":"2023-04-27T21:21:48.849600Z","shell.execute_reply":"2023-04-27T21:21:49.010037Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"               0           1           2           3           4           5   \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.029164    0.038437    0.043832    0.053892    0.075202    0.104570   \nstd      0.022991    0.032960    0.038428    0.046528    0.055552    0.059105   \nmin      0.001500    0.000600    0.001500    0.005800    0.006700    0.010200   \n25%      0.013350    0.016450    0.018950    0.024375    0.038050    0.067025   \n50%      0.022800    0.030800    0.034300    0.044050    0.062500    0.092150   \n75%      0.035550    0.047950    0.057950    0.064500    0.100275    0.134125   \nmax      0.137100    0.233900    0.305900    0.426400    0.401000    0.382300   \n\n               6           7           8           9   ...          50  \\\ncount  208.000000  208.000000  208.000000  208.000000  ...  208.000000   \nmean     0.121747    0.134799    0.178003    0.208259  ...    0.016069   \nstd      0.061788    0.085152    0.118387    0.134416  ...    0.012008   \nmin      0.003300    0.005500    0.007500    0.011300  ...    0.000000   \n25%      0.080900    0.080425    0.097025    0.111275  ...    0.008425   \n50%      0.106950    0.112100    0.152250    0.182400  ...    0.013900   \n75%      0.154000    0.169600    0.233425    0.268700  ...    0.020825   \nmax      0.372900    0.459000    0.682800    0.710600  ...    0.100400   \n\n               51          52          53          54          55          56  \\\ncount  208.000000  208.000000  208.000000  208.000000  208.000000  208.000000   \nmean     0.013420    0.010709    0.010941    0.009290    0.008222    0.007820   \nstd      0.009634    0.007060    0.007301    0.007088    0.005736    0.005785   \nmin      0.000800    0.000500    0.001000    0.000600    0.000400    0.000300   \n25%      0.007275    0.005075    0.005375    0.004150    0.004400    0.003700   \n50%      0.011400    0.009550    0.009300    0.007500    0.006850    0.005950   \n75%      0.016725    0.014900    0.014500    0.012100    0.010575    0.010425   \nmax      0.070900    0.039000    0.035200    0.044700    0.039400    0.035500   \n\n               57          58          59  \ncount  208.000000  208.000000  208.000000  \nmean     0.007949    0.007941    0.006507  \nstd      0.006470    0.006181    0.005031  \nmin      0.000300    0.000100    0.000600  \n25%      0.003600    0.003675    0.003100  \n50%      0.005800    0.006400    0.005300  \n75%      0.010350    0.010325    0.008525  \nmax      0.044000    0.036400    0.043900  \n\n[8 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>...</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n      <td>208.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.029164</td>\n      <td>0.038437</td>\n      <td>0.043832</td>\n      <td>0.053892</td>\n      <td>0.075202</td>\n      <td>0.104570</td>\n      <td>0.121747</td>\n      <td>0.134799</td>\n      <td>0.178003</td>\n      <td>0.208259</td>\n      <td>...</td>\n      <td>0.016069</td>\n      <td>0.013420</td>\n      <td>0.010709</td>\n      <td>0.010941</td>\n      <td>0.009290</td>\n      <td>0.008222</td>\n      <td>0.007820</td>\n      <td>0.007949</td>\n      <td>0.007941</td>\n      <td>0.006507</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.022991</td>\n      <td>0.032960</td>\n      <td>0.038428</td>\n      <td>0.046528</td>\n      <td>0.055552</td>\n      <td>0.059105</td>\n      <td>0.061788</td>\n      <td>0.085152</td>\n      <td>0.118387</td>\n      <td>0.134416</td>\n      <td>...</td>\n      <td>0.012008</td>\n      <td>0.009634</td>\n      <td>0.007060</td>\n      <td>0.007301</td>\n      <td>0.007088</td>\n      <td>0.005736</td>\n      <td>0.005785</td>\n      <td>0.006470</td>\n      <td>0.006181</td>\n      <td>0.005031</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.001500</td>\n      <td>0.000600</td>\n      <td>0.001500</td>\n      <td>0.005800</td>\n      <td>0.006700</td>\n      <td>0.010200</td>\n      <td>0.003300</td>\n      <td>0.005500</td>\n      <td>0.007500</td>\n      <td>0.011300</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000800</td>\n      <td>0.000500</td>\n      <td>0.001000</td>\n      <td>0.000600</td>\n      <td>0.000400</td>\n      <td>0.000300</td>\n      <td>0.000300</td>\n      <td>0.000100</td>\n      <td>0.000600</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.013350</td>\n      <td>0.016450</td>\n      <td>0.018950</td>\n      <td>0.024375</td>\n      <td>0.038050</td>\n      <td>0.067025</td>\n      <td>0.080900</td>\n      <td>0.080425</td>\n      <td>0.097025</td>\n      <td>0.111275</td>\n      <td>...</td>\n      <td>0.008425</td>\n      <td>0.007275</td>\n      <td>0.005075</td>\n      <td>0.005375</td>\n      <td>0.004150</td>\n      <td>0.004400</td>\n      <td>0.003700</td>\n      <td>0.003600</td>\n      <td>0.003675</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.022800</td>\n      <td>0.030800</td>\n      <td>0.034300</td>\n      <td>0.044050</td>\n      <td>0.062500</td>\n      <td>0.092150</td>\n      <td>0.106950</td>\n      <td>0.112100</td>\n      <td>0.152250</td>\n      <td>0.182400</td>\n      <td>...</td>\n      <td>0.013900</td>\n      <td>0.011400</td>\n      <td>0.009550</td>\n      <td>0.009300</td>\n      <td>0.007500</td>\n      <td>0.006850</td>\n      <td>0.005950</td>\n      <td>0.005800</td>\n      <td>0.006400</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.035550</td>\n      <td>0.047950</td>\n      <td>0.057950</td>\n      <td>0.064500</td>\n      <td>0.100275</td>\n      <td>0.134125</td>\n      <td>0.154000</td>\n      <td>0.169600</td>\n      <td>0.233425</td>\n      <td>0.268700</td>\n      <td>...</td>\n      <td>0.020825</td>\n      <td>0.016725</td>\n      <td>0.014900</td>\n      <td>0.014500</td>\n      <td>0.012100</td>\n      <td>0.010575</td>\n      <td>0.010425</td>\n      <td>0.010350</td>\n      <td>0.010325</td>\n      <td>0.008525</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.137100</td>\n      <td>0.233900</td>\n      <td>0.305900</td>\n      <td>0.426400</td>\n      <td>0.401000</td>\n      <td>0.382300</td>\n      <td>0.372900</td>\n      <td>0.459000</td>\n      <td>0.682800</td>\n      <td>0.710600</td>\n      <td>...</td>\n      <td>0.100400</td>\n      <td>0.070900</td>\n      <td>0.039000</td>\n      <td>0.035200</td>\n      <td>0.044700</td>\n      <td>0.039400</td>\n      <td>0.035500</td>\n      <td>0.044000</td>\n      <td>0.036400</td>\n      <td>0.043900</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Lets see the number of rows and colums","metadata":{}},{"cell_type":"code","source":"sonar_data.shape\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.012899Z","iopub.execute_input":"2023-04-27T21:21:49.014327Z","iopub.status.idle":"2023-04-27T21:21:49.023119Z","shell.execute_reply.started":"2023-04-27T21:21:49.014271Z","shell.execute_reply":"2023-04-27T21:21:49.022007Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(208, 61)"},"metadata":{}}]},{"cell_type":"markdown","source":"Lets see what the range of data is","metadata":{}},{"cell_type":"code","source":"print(f\"The min value in sonar_data is {min(sonar_data.describe().loc['min'])}\\nThe max value in sonar_data is:{max(sonar_data.describe().loc['max'])}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.024696Z","iopub.execute_input":"2023-04-27T21:21:49.025368Z","iopub.status.idle":"2023-04-27T21:21:49.289331Z","shell.execute_reply.started":"2023-04-27T21:21:49.025303Z","shell.execute_reply":"2023-04-27T21:21:49.287742Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"The min value in sonar_data is 0.0\nThe max value in sonar_data is:1.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**We can see this dataset has 61 columns, 59 are numerical dependent variables and 1 independent variable “R” or “M” rock or mine. There are 208 lines of data. Looks like the features of the dataset are already normalized between 0 and 1.**","metadata":{}},{"cell_type":"markdown","source":"## Step 3: Preprocess the Data\n\n**This dataset is in good shape since it's already normalized. We should convert the string \"R\" and \"M\" to numeric values to aid in prediction. We can use 1 for rock (R) and -1 for mine (M).**","metadata":{}},{"cell_type":"markdown","source":"Lets to a look at the dataset before the change","metadata":{}},{"cell_type":"code","source":"sonar_data.iloc[:,60]","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.291343Z","iopub.execute_input":"2023-04-27T21:21:49.291729Z","iopub.status.idle":"2023-04-27T21:21:49.301570Z","shell.execute_reply.started":"2023-04-27T21:21:49.291693Z","shell.execute_reply":"2023-04-27T21:21:49.300089Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"0      R\n1      R\n2      R\n3      R\n4      R\n      ..\n203    M\n204    M\n205    M\n206    M\n207    M\nName: 60, Length: 208, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"we can see the column 60 is string values R and M","metadata":{}},{"cell_type":"markdown","source":"Now we can use the numpy where function to conver these values to numerical representations and then we will use the same code as above to confirm the string to int conversion","metadata":{}},{"cell_type":"code","source":"sonar_data.iloc[:, 60] = np.where(sonar_data.iloc[:, 60] == \"R\", 1, np.where(sonar_data.iloc[:, 60] == \"M\", -1, sonar_data.iloc[:, 60]))\nsonar_data.iloc[:,60]","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.303509Z","iopub.execute_input":"2023-04-27T21:21:49.303991Z","iopub.status.idle":"2023-04-27T21:21:49.320130Z","shell.execute_reply.started":"2023-04-27T21:21:49.303942Z","shell.execute_reply":"2023-04-27T21:21:49.318622Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"0       1\n1       1\n2       1\n3       1\n4       1\n       ..\n203    -1\n204    -1\n205    -1\n206    -1\n207    -1\nName: 60, Length: 208, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"**The results of converting R&M to 1 or - 1 will be useful in order to train the perceptron and adjust the weight biases. We will need to be aware of this change and correct for it back into the R & M format if reporting any results to make them meaningful outcomes of the model.**","metadata":{}},{"cell_type":"markdown","source":"## Step 4: Prepare the Training and Test Data Sets\n**We can use the train test split function from SK learn to separate the sonar data into a training and testing data set we'll want to be conscious that the output (target) variable is the last variable of the data set and the features are all other columns. The train split function will split that 208 rows of data appropriately and we can use a 80:20 ratio.**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\n# Separate features and target columns\nX = sonar_data.iloc[:, :-1]\ny = sonar_data.iloc[:, -1]\ny = y.astype(int)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.321733Z","iopub.execute_input":"2023-04-27T21:21:49.322102Z","iopub.status.idle":"2023-04-27T21:21:49.333060Z","shell.execute_reply.started":"2023-04-27T21:21:49.322064Z","shell.execute_reply":"2023-04-27T21:21:49.331533Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"lets do a sanity check and use head and shape to examine our split for size and randomness before proceeding","metadata":{}},{"cell_type":"code","source":"X_train.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.334556Z","iopub.execute_input":"2023-04-27T21:21:49.334946Z","iopub.status.idle":"2023-04-27T21:21:49.372108Z","shell.execute_reply.started":"2023-04-27T21:21:49.334911Z","shell.execute_reply":"2023-04-27T21:21:49.370595Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"         0       1       2       3       4       5       6       7       8   \\\n86   0.0188  0.0370  0.0953  0.0824  0.0249  0.0488  0.1424  0.1972  0.1873   \n203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  0.1694  0.2328   \n67   0.0368  0.0403  0.0317  0.0293  0.0820  0.1342  0.1161  0.0663  0.0155   \n82   0.0409  0.0421  0.0573  0.0130  0.0183  0.1019  0.1054  0.1070  0.2302   \n205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  0.1178  0.1258   \n194  0.0392  0.0108  0.0267  0.0257  0.0410  0.0491  0.1053  0.1690  0.2105   \n38   0.0123  0.0022  0.0196  0.0206  0.0180  0.0492  0.0033  0.0398  0.0791   \n24   0.0293  0.0644  0.0390  0.0173  0.0476  0.0816  0.0993  0.0315  0.0736   \n60   0.0130  0.0006  0.0088  0.0456  0.0525  0.0778  0.0931  0.0941  0.1711   \n195  0.0129  0.0141  0.0309  0.0375  0.0767  0.0787  0.0662  0.1108  0.1777   \n\n         9   ...      50      51      52      53      54      55      56  \\\n86   0.1806  ...  0.0143  0.0093  0.0033  0.0113  0.0030  0.0057  0.0090   \n203  0.2684  ...  0.0203  0.0116  0.0098  0.0199  0.0033  0.0101  0.0065   \n67   0.0506  ...  0.0058  0.0091  0.0160  0.0160  0.0081  0.0070  0.0135   \n82   0.2259  ...  0.0113  0.0028  0.0036  0.0105  0.0120  0.0087  0.0061   \n205  0.2529  ...  0.0155  0.0160  0.0029  0.0051  0.0062  0.0089  0.0140   \n194  0.2471  ...  0.0089  0.0083  0.0080  0.0026  0.0079  0.0042  0.0071   \n38   0.0475  ...  0.0149  0.0125  0.0134  0.0026  0.0038  0.0018  0.0113   \n24   0.0860  ...  0.0170  0.0035  0.0052  0.0083  0.0078  0.0075  0.0105   \n60   0.1483  ...  0.0092  0.0078  0.0041  0.0013  0.0011  0.0045  0.0039   \n195  0.2245  ...  0.0204  0.0124  0.0093  0.0072  0.0019  0.0027  0.0054   \n\n         57      58      59  \n86   0.0057  0.0068  0.0024  \n203  0.0115  0.0193  0.0157  \n67   0.0067  0.0078  0.0068  \n82   0.0061  0.0030  0.0078  \n205  0.0138  0.0077  0.0031  \n194  0.0044  0.0022  0.0014  \n38   0.0058  0.0047  0.0071  \n24   0.0160  0.0095  0.0011  \n60   0.0022  0.0023  0.0016  \n195  0.0017  0.0024  0.0029  \n\n[10 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>86</th>\n      <td>0.0188</td>\n      <td>0.0370</td>\n      <td>0.0953</td>\n      <td>0.0824</td>\n      <td>0.0249</td>\n      <td>0.0488</td>\n      <td>0.1424</td>\n      <td>0.1972</td>\n      <td>0.1873</td>\n      <td>0.1806</td>\n      <td>...</td>\n      <td>0.0143</td>\n      <td>0.0093</td>\n      <td>0.0033</td>\n      <td>0.0113</td>\n      <td>0.0030</td>\n      <td>0.0057</td>\n      <td>0.0090</td>\n      <td>0.0057</td>\n      <td>0.0068</td>\n      <td>0.0024</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.0187</td>\n      <td>0.0346</td>\n      <td>0.0168</td>\n      <td>0.0177</td>\n      <td>0.0393</td>\n      <td>0.1630</td>\n      <td>0.2028</td>\n      <td>0.1694</td>\n      <td>0.2328</td>\n      <td>0.2684</td>\n      <td>...</td>\n      <td>0.0203</td>\n      <td>0.0116</td>\n      <td>0.0098</td>\n      <td>0.0199</td>\n      <td>0.0033</td>\n      <td>0.0101</td>\n      <td>0.0065</td>\n      <td>0.0115</td>\n      <td>0.0193</td>\n      <td>0.0157</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>0.0368</td>\n      <td>0.0403</td>\n      <td>0.0317</td>\n      <td>0.0293</td>\n      <td>0.0820</td>\n      <td>0.1342</td>\n      <td>0.1161</td>\n      <td>0.0663</td>\n      <td>0.0155</td>\n      <td>0.0506</td>\n      <td>...</td>\n      <td>0.0058</td>\n      <td>0.0091</td>\n      <td>0.0160</td>\n      <td>0.0160</td>\n      <td>0.0081</td>\n      <td>0.0070</td>\n      <td>0.0135</td>\n      <td>0.0067</td>\n      <td>0.0078</td>\n      <td>0.0068</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>0.0409</td>\n      <td>0.0421</td>\n      <td>0.0573</td>\n      <td>0.0130</td>\n      <td>0.0183</td>\n      <td>0.1019</td>\n      <td>0.1054</td>\n      <td>0.1070</td>\n      <td>0.2302</td>\n      <td>0.2259</td>\n      <td>...</td>\n      <td>0.0113</td>\n      <td>0.0028</td>\n      <td>0.0036</td>\n      <td>0.0105</td>\n      <td>0.0120</td>\n      <td>0.0087</td>\n      <td>0.0061</td>\n      <td>0.0061</td>\n      <td>0.0030</td>\n      <td>0.0078</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.0522</td>\n      <td>0.0437</td>\n      <td>0.0180</td>\n      <td>0.0292</td>\n      <td>0.0351</td>\n      <td>0.1171</td>\n      <td>0.1257</td>\n      <td>0.1178</td>\n      <td>0.1258</td>\n      <td>0.2529</td>\n      <td>...</td>\n      <td>0.0155</td>\n      <td>0.0160</td>\n      <td>0.0029</td>\n      <td>0.0051</td>\n      <td>0.0062</td>\n      <td>0.0089</td>\n      <td>0.0140</td>\n      <td>0.0138</td>\n      <td>0.0077</td>\n      <td>0.0031</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>0.0392</td>\n      <td>0.0108</td>\n      <td>0.0267</td>\n      <td>0.0257</td>\n      <td>0.0410</td>\n      <td>0.0491</td>\n      <td>0.1053</td>\n      <td>0.1690</td>\n      <td>0.2105</td>\n      <td>0.2471</td>\n      <td>...</td>\n      <td>0.0089</td>\n      <td>0.0083</td>\n      <td>0.0080</td>\n      <td>0.0026</td>\n      <td>0.0079</td>\n      <td>0.0042</td>\n      <td>0.0071</td>\n      <td>0.0044</td>\n      <td>0.0022</td>\n      <td>0.0014</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.0123</td>\n      <td>0.0022</td>\n      <td>0.0196</td>\n      <td>0.0206</td>\n      <td>0.0180</td>\n      <td>0.0492</td>\n      <td>0.0033</td>\n      <td>0.0398</td>\n      <td>0.0791</td>\n      <td>0.0475</td>\n      <td>...</td>\n      <td>0.0149</td>\n      <td>0.0125</td>\n      <td>0.0134</td>\n      <td>0.0026</td>\n      <td>0.0038</td>\n      <td>0.0018</td>\n      <td>0.0113</td>\n      <td>0.0058</td>\n      <td>0.0047</td>\n      <td>0.0071</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.0293</td>\n      <td>0.0644</td>\n      <td>0.0390</td>\n      <td>0.0173</td>\n      <td>0.0476</td>\n      <td>0.0816</td>\n      <td>0.0993</td>\n      <td>0.0315</td>\n      <td>0.0736</td>\n      <td>0.0860</td>\n      <td>...</td>\n      <td>0.0170</td>\n      <td>0.0035</td>\n      <td>0.0052</td>\n      <td>0.0083</td>\n      <td>0.0078</td>\n      <td>0.0075</td>\n      <td>0.0105</td>\n      <td>0.0160</td>\n      <td>0.0095</td>\n      <td>0.0011</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>0.0130</td>\n      <td>0.0006</td>\n      <td>0.0088</td>\n      <td>0.0456</td>\n      <td>0.0525</td>\n      <td>0.0778</td>\n      <td>0.0931</td>\n      <td>0.0941</td>\n      <td>0.1711</td>\n      <td>0.1483</td>\n      <td>...</td>\n      <td>0.0092</td>\n      <td>0.0078</td>\n      <td>0.0041</td>\n      <td>0.0013</td>\n      <td>0.0011</td>\n      <td>0.0045</td>\n      <td>0.0039</td>\n      <td>0.0022</td>\n      <td>0.0023</td>\n      <td>0.0016</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>0.0129</td>\n      <td>0.0141</td>\n      <td>0.0309</td>\n      <td>0.0375</td>\n      <td>0.0767</td>\n      <td>0.0787</td>\n      <td>0.0662</td>\n      <td>0.1108</td>\n      <td>0.1777</td>\n      <td>0.2245</td>\n      <td>...</td>\n      <td>0.0204</td>\n      <td>0.0124</td>\n      <td>0.0093</td>\n      <td>0.0072</td>\n      <td>0.0019</td>\n      <td>0.0027</td>\n      <td>0.0054</td>\n      <td>0.0017</td>\n      <td>0.0024</td>\n      <td>0.0029</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.373940Z","iopub.execute_input":"2023-04-27T21:21:49.374387Z","iopub.status.idle":"2023-04-27T21:21:49.384564Z","shell.execute_reply.started":"2023-04-27T21:21:49.374334Z","shell.execute_reply":"2023-04-27T21:21:49.383091Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"(166, 60)"},"metadata":{}}]},{"cell_type":"code","source":"X_test.head(n=10)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.391641Z","iopub.execute_input":"2023-04-27T21:21:49.394377Z","iopub.status.idle":"2023-04-27T21:21:49.430437Z","shell.execute_reply.started":"2023-04-27T21:21:49.394329Z","shell.execute_reply":"2023-04-27T21:21:49.428717Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"         0       1       2       3       4       5       6       7       8   \\\n161  0.0305  0.0363  0.0214  0.0227  0.0456  0.0665  0.0939  0.0972  0.2535   \n15   0.0298  0.0615  0.0650  0.0921  0.1615  0.2294  0.2176  0.2033  0.1459   \n73   0.0139  0.0222  0.0089  0.0108  0.0215  0.0136  0.0659  0.0954  0.0786   \n96   0.0181  0.0146  0.0026  0.0141  0.0421  0.0473  0.0361  0.0741  0.1398   \n166  0.0411  0.0277  0.0604  0.0525  0.0489  0.0385  0.0611  0.1117  0.1237   \n9    0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n100  0.0629  0.1065  0.1526  0.1229  0.1437  0.1190  0.0884  0.0907  0.2107   \n135  0.0094  0.0611  0.1136  0.1203  0.0403  0.1227  0.2495  0.4566  0.6587   \n18   0.0270  0.0092  0.0145  0.0278  0.0412  0.0757  0.1026  0.1138  0.0794   \n148  0.0712  0.0901  0.1276  0.1497  0.1284  0.1165  0.1285  0.1684  0.1830   \n\n         9   ...      50      51      52      53      54      55      56  \\\n161  0.3127  ...  0.0271  0.0200  0.0070  0.0070  0.0086  0.0089  0.0074   \n15   0.0852  ...  0.0154  0.0031  0.0153  0.0071  0.0212  0.0076  0.0152   \n73   0.1015  ...  0.0024  0.0062  0.0072  0.0113  0.0012  0.0022  0.0025   \n96   0.1045  ...  0.0076  0.0223  0.0255  0.0145  0.0233  0.0041  0.0018   \n166  0.2300  ...  0.0181  0.0217  0.0038  0.0019  0.0065  0.0132  0.0108   \n9    0.0251  ...  0.0118  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032   \n100  0.3597  ...  0.0257  0.0089  0.0262  0.0108  0.0138  0.0187  0.0230   \n135  0.5079  ...  0.0480  0.0234  0.0175  0.0352  0.0158  0.0326  0.0201   \n18   0.1520  ...  0.0045  0.0084  0.0010  0.0018  0.0068  0.0039  0.0120   \n148  0.2127  ...  0.0154  0.0154  0.0156  0.0054  0.0030  0.0048  0.0087   \n\n         57      58      59  \n161  0.0042  0.0055  0.0021  \n15   0.0049  0.0200  0.0073  \n73   0.0059  0.0039  0.0048  \n96   0.0048  0.0089  0.0085  \n166  0.0050  0.0085  0.0044  \n9    0.0035  0.0056  0.0040  \n100  0.0057  0.0113  0.0131  \n135  0.0168  0.0245  0.0154  \n18   0.0132  0.0070  0.0088  \n148  0.0101  0.0095  0.0068  \n\n[10 rows x 60 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n      <th>56</th>\n      <th>57</th>\n      <th>58</th>\n      <th>59</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>161</th>\n      <td>0.0305</td>\n      <td>0.0363</td>\n      <td>0.0214</td>\n      <td>0.0227</td>\n      <td>0.0456</td>\n      <td>0.0665</td>\n      <td>0.0939</td>\n      <td>0.0972</td>\n      <td>0.2535</td>\n      <td>0.3127</td>\n      <td>...</td>\n      <td>0.0271</td>\n      <td>0.0200</td>\n      <td>0.0070</td>\n      <td>0.0070</td>\n      <td>0.0086</td>\n      <td>0.0089</td>\n      <td>0.0074</td>\n      <td>0.0042</td>\n      <td>0.0055</td>\n      <td>0.0021</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.0298</td>\n      <td>0.0615</td>\n      <td>0.0650</td>\n      <td>0.0921</td>\n      <td>0.1615</td>\n      <td>0.2294</td>\n      <td>0.2176</td>\n      <td>0.2033</td>\n      <td>0.1459</td>\n      <td>0.0852</td>\n      <td>...</td>\n      <td>0.0154</td>\n      <td>0.0031</td>\n      <td>0.0153</td>\n      <td>0.0071</td>\n      <td>0.0212</td>\n      <td>0.0076</td>\n      <td>0.0152</td>\n      <td>0.0049</td>\n      <td>0.0200</td>\n      <td>0.0073</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>0.0139</td>\n      <td>0.0222</td>\n      <td>0.0089</td>\n      <td>0.0108</td>\n      <td>0.0215</td>\n      <td>0.0136</td>\n      <td>0.0659</td>\n      <td>0.0954</td>\n      <td>0.0786</td>\n      <td>0.1015</td>\n      <td>...</td>\n      <td>0.0024</td>\n      <td>0.0062</td>\n      <td>0.0072</td>\n      <td>0.0113</td>\n      <td>0.0012</td>\n      <td>0.0022</td>\n      <td>0.0025</td>\n      <td>0.0059</td>\n      <td>0.0039</td>\n      <td>0.0048</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>0.0181</td>\n      <td>0.0146</td>\n      <td>0.0026</td>\n      <td>0.0141</td>\n      <td>0.0421</td>\n      <td>0.0473</td>\n      <td>0.0361</td>\n      <td>0.0741</td>\n      <td>0.1398</td>\n      <td>0.1045</td>\n      <td>...</td>\n      <td>0.0076</td>\n      <td>0.0223</td>\n      <td>0.0255</td>\n      <td>0.0145</td>\n      <td>0.0233</td>\n      <td>0.0041</td>\n      <td>0.0018</td>\n      <td>0.0048</td>\n      <td>0.0089</td>\n      <td>0.0085</td>\n    </tr>\n    <tr>\n      <th>166</th>\n      <td>0.0411</td>\n      <td>0.0277</td>\n      <td>0.0604</td>\n      <td>0.0525</td>\n      <td>0.0489</td>\n      <td>0.0385</td>\n      <td>0.0611</td>\n      <td>0.1117</td>\n      <td>0.1237</td>\n      <td>0.2300</td>\n      <td>...</td>\n      <td>0.0181</td>\n      <td>0.0217</td>\n      <td>0.0038</td>\n      <td>0.0019</td>\n      <td>0.0065</td>\n      <td>0.0132</td>\n      <td>0.0108</td>\n      <td>0.0050</td>\n      <td>0.0085</td>\n      <td>0.0044</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0164</td>\n      <td>0.0173</td>\n      <td>0.0347</td>\n      <td>0.0070</td>\n      <td>0.0187</td>\n      <td>0.0671</td>\n      <td>0.1056</td>\n      <td>0.0697</td>\n      <td>0.0962</td>\n      <td>0.0251</td>\n      <td>...</td>\n      <td>0.0118</td>\n      <td>0.0090</td>\n      <td>0.0223</td>\n      <td>0.0179</td>\n      <td>0.0084</td>\n      <td>0.0068</td>\n      <td>0.0032</td>\n      <td>0.0035</td>\n      <td>0.0056</td>\n      <td>0.0040</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.0629</td>\n      <td>0.1065</td>\n      <td>0.1526</td>\n      <td>0.1229</td>\n      <td>0.1437</td>\n      <td>0.1190</td>\n      <td>0.0884</td>\n      <td>0.0907</td>\n      <td>0.2107</td>\n      <td>0.3597</td>\n      <td>...</td>\n      <td>0.0257</td>\n      <td>0.0089</td>\n      <td>0.0262</td>\n      <td>0.0108</td>\n      <td>0.0138</td>\n      <td>0.0187</td>\n      <td>0.0230</td>\n      <td>0.0057</td>\n      <td>0.0113</td>\n      <td>0.0131</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>0.0094</td>\n      <td>0.0611</td>\n      <td>0.1136</td>\n      <td>0.1203</td>\n      <td>0.0403</td>\n      <td>0.1227</td>\n      <td>0.2495</td>\n      <td>0.4566</td>\n      <td>0.6587</td>\n      <td>0.5079</td>\n      <td>...</td>\n      <td>0.0480</td>\n      <td>0.0234</td>\n      <td>0.0175</td>\n      <td>0.0352</td>\n      <td>0.0158</td>\n      <td>0.0326</td>\n      <td>0.0201</td>\n      <td>0.0168</td>\n      <td>0.0245</td>\n      <td>0.0154</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.0270</td>\n      <td>0.0092</td>\n      <td>0.0145</td>\n      <td>0.0278</td>\n      <td>0.0412</td>\n      <td>0.0757</td>\n      <td>0.1026</td>\n      <td>0.1138</td>\n      <td>0.0794</td>\n      <td>0.1520</td>\n      <td>...</td>\n      <td>0.0045</td>\n      <td>0.0084</td>\n      <td>0.0010</td>\n      <td>0.0018</td>\n      <td>0.0068</td>\n      <td>0.0039</td>\n      <td>0.0120</td>\n      <td>0.0132</td>\n      <td>0.0070</td>\n      <td>0.0088</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0.0712</td>\n      <td>0.0901</td>\n      <td>0.1276</td>\n      <td>0.1497</td>\n      <td>0.1284</td>\n      <td>0.1165</td>\n      <td>0.1285</td>\n      <td>0.1684</td>\n      <td>0.1830</td>\n      <td>0.2127</td>\n      <td>...</td>\n      <td>0.0154</td>\n      <td>0.0154</td>\n      <td>0.0156</td>\n      <td>0.0054</td>\n      <td>0.0030</td>\n      <td>0.0048</td>\n      <td>0.0087</td>\n      <td>0.0101</td>\n      <td>0.0095</td>\n      <td>0.0068</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 60 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"X_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.432779Z","iopub.execute_input":"2023-04-27T21:21:49.433304Z","iopub.status.idle":"2023-04-27T21:21:49.441675Z","shell.execute_reply.started":"2023-04-27T21:21:49.433252Z","shell.execute_reply":"2023-04-27T21:21:49.440179Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"(42, 60)"},"metadata":{}}]},{"cell_type":"code","source":"y_test[0:9]","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.443206Z","iopub.execute_input":"2023-04-27T21:21:49.443610Z","iopub.status.idle":"2023-04-27T21:21:49.456168Z","shell.execute_reply.started":"2023-04-27T21:21:49.443571Z","shell.execute_reply":"2023-04-27T21:21:49.454447Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"161   -1\n15     1\n73     1\n96     1\n166   -1\n9      1\n100   -1\n135   -1\n18     1\nName: 60, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"y_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.458900Z","iopub.execute_input":"2023-04-27T21:21:49.459318Z","iopub.status.idle":"2023-04-27T21:21:49.467652Z","shell.execute_reply.started":"2023-04-27T21:21:49.459280Z","shell.execute_reply":"2023-04-27T21:21:49.466303Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"(42,)"},"metadata":{}}]},{"cell_type":"code","source":"y_train[0:9]","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.470174Z","iopub.execute_input":"2023-04-27T21:21:49.470607Z","iopub.status.idle":"2023-04-27T21:21:49.480569Z","shell.execute_reply.started":"2023-04-27T21:21:49.470557Z","shell.execute_reply":"2023-04-27T21:21:49.479370Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"86     1\n203   -1\n67     1\n82     1\n205   -1\n194   -1\n38     1\n24     1\n60     1\nName: 60, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"**We successfully split the training and test data based on 80:20 rule and divided the feature verse outcome.  We verified it this split and saw that X_train has 166 values and X_test has 42 rows the same distribution occurred with the Y test and train. The X data features the 59 records and the y data is only 1 and - 1 outcomes as we expect.**","metadata":{}},{"cell_type":"markdown","source":"## Step 5: Instantiate and Configure a Perceptron\n**In this step we will initiate and configure a perceptron model from SK learn and define a few of the parameters within the model**","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n#help(Perceptron)\nperceptron = Perceptron(alpha = 0.1, max_iter = 10000)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.483762Z","iopub.execute_input":"2023-04-27T21:21:49.484275Z","iopub.status.idle":"2023-04-27T21:21:49.491912Z","shell.execute_reply.started":"2023-04-27T21:21:49.484225Z","shell.execute_reply":"2023-04-27T21:21:49.490679Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"**following prior examples we set alpha to .01 in the max_itter or number of epochs to 10,000 this should serve as good base model parameters but if convergence does not occur we can expand the number of epochs or tweak the settings otherwise of the model**","metadata":{}},{"cell_type":"markdown","source":"## Step 6: Train the Perceptron\n**We will train the perceptron model with the X and y train datasets using the fit metho**","metadata":{}},{"cell_type":"code","source":"# Convert varriables to inttype\ny_train = y_train.astype(int)\ny_test = y_test.astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.493702Z","iopub.execute_input":"2023-04-27T21:21:49.494541Z","iopub.status.idle":"2023-04-27T21:21:49.503945Z","shell.execute_reply.started":"2023-04-27T21:21:49.494486Z","shell.execute_reply":"2023-04-27T21:21:49.502913Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"#help(perceptron.fit)\nperceptron.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.505360Z","iopub.execute_input":"2023-04-27T21:21:49.506522Z","iopub.status.idle":"2023-04-27T21:21:49.529618Z","shell.execute_reply.started":"2023-04-27T21:21:49.506469Z","shell.execute_reply":"2023-04-27T21:21:49.528135Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"Perceptron(alpha=0.1, max_iter=10000)"},"metadata":{}}]},{"cell_type":"markdown","source":"**we discovered that the variable types needed to be converted to integers prior to training the model otherwise the perceptron fit method worked just fine given our parameters of alpha and number of epochs**","metadata":{}},{"cell_type":"markdown","source":"## Step 7: Test the Perceptron\n\n**In this section we will test the perceptron using the score function and report the model's accuracy on the test data set**\n","metadata":{}},{"cell_type":"code","source":"accuracy = perceptron.score(X_test, y_test)\nprint(f'Test accuracy: {accuracy * 100:.1f}%')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.531674Z","iopub.execute_input":"2023-04-27T21:21:49.532179Z","iopub.status.idle":"2023-04-27T21:21:49.543258Z","shell.execute_reply.started":"2023-04-27T21:21:49.532115Z","shell.execute_reply":"2023-04-27T21:21:49.541943Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Test accuracy: 88.1%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**We can see that the accuracy of the perceptron is 88.1% on the test data set this seems like a  reasonable number for a perceptron model as high 90’s would be a read flag, and of course very low numbers would indicate problems**","metadata":{}},{"cell_type":"markdown","source":"## Step 8: Demonstrate Making Predictions\n**Here we can use the predict function to convert the numeric features of a small number of X_test data into outcomes and ultimately return what they're not it is classified as Rock or Mine**","metadata":{}},{"cell_type":"code","source":"results = perceptron.predict(X_test.head(n=10))\nfor i in results:\n    rock_mine_pred = np.where(results == 1, \"Rock\", np.where(results == -1, \"Mine\", results))\nprint(rock_mine_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:49.545250Z","iopub.execute_input":"2023-04-27T21:21:49.545742Z","iopub.status.idle":"2023-04-27T21:21:49.565251Z","shell.execute_reply.started":"2023-04-27T21:21:49.545678Z","shell.execute_reply":"2023-04-27T21:21:49.562382Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"['Mine' 'Rock' 'Rock' 'Rock' 'Mine' 'Rock' 'Mine' 'Mine' 'Rock' 'Mine']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Here we used the first 10 lines in the X_test data set to demonstrate the perceptrons prediction ability based off the features of the test data set after we perform this prediction we wanted to convert the numeric results back to the text Rock or Mine values and print those**","metadata":{}},{"cell_type":"markdown","source":"## Step 9: Evaluate (and Improve?)\n\nDescribe the configuration and performance of your classifier, and what the results mean. Is this a good classifier? Why isn't it better than what it is? What might you try next to improve it? How accurate can you make your classifier? If you have time, see if you can increase the accuracy. Can you beat 98%? 🙀\n\n**The classifier is configured as a linear model with alpha of 0.01 and maximum number of epochs set to 10,000.  The classifier worked well given the 88.1% accuracy. There is an limitation to the accuracy that can be achieved and trying to improve this through hyper parameterization since a linear classifier was chosen if a different type of classifier was chosen a greater accuracy could potentially be achieved through hyper parameterization improving the accuracy was achieved only by reducing the features of the dataset to 30. Manipulating the training test split ratio changed the accuracy similar to cross validation but greater than 88.1% could not be achieved. No beating 98% is not possible given the model selected.**","metadata":{}},{"cell_type":"markdown","source":"Can we Manipulating the alpha paramater to improve the model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n\noptimized_perceptron = Perceptron(alpha=0.0001, max_iter=10000, penalty= 'none')\noptimized_perceptron.fit(X_train, y_train)\noptimize_accuracy = optimized_perceptron.score(X_test, y_test)\nprint(f'Test accuracy: {optimize_accuracy  * 100:.1f}%') #Test accuracy: 88.1%\n\noptimized_perceptron = Perceptron(alpha=0.9, max_iter=10000, penalty= 'none')\noptimized_perceptron.fit(X_train, y_train)\noptimize_accuracy = optimized_perceptron.score(X_test, y_test)\nprint(f'Test accuracy: {optimize_accuracy  * 100:.1f}%') #Test accuracy: 88.1% ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:26:01.407700Z","iopub.execute_input":"2023-04-27T21:26:01.408285Z","iopub.status.idle":"2023-04-27T21:26:01.433987Z","shell.execute_reply.started":"2023-04-27T21:26:01.408239Z","shell.execute_reply":"2023-04-27T21:26:01.432686Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Test accuracy: 88.1%\nTest accuracy: 88.1%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see the range of 0.0001 to 0.9 for alpha did not change the models accuracy","metadata":{}},{"cell_type":"markdown","source":"Can we Manipulating the max_iter paramater to improve the model","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\noptimized_perceptron = Perceptron(alpha=0.1, max_iter=1000, penalty= 'none')\noptimized_perceptron.fit(X_train, y_train)\noptimize_accuracy = optimized_perceptron.score(X_test, y_test)\nprint(f'Test accuracy: {optimize_accuracy  * 100:.1f}%') #Test accuracy: 88.1%\n\noptimized_perceptron = Perceptron(alpha=0.1, max_iter=1000000, penalty= 'none')\noptimized_perceptron.fit(X_train, y_train)\noptimize_accuracy = optimized_perceptron.score(X_test, y_test)\nprint(f'Test accuracy: {optimize_accuracy  * 100:.1f}%') #Test accuracy: 88.1%","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:26:10.764670Z","iopub.execute_input":"2023-04-27T21:26:10.765218Z","iopub.status.idle":"2023-04-27T21:26:10.790956Z","shell.execute_reply.started":"2023-04-27T21:26:10.765156Z","shell.execute_reply":"2023-04-27T21:26:10.789783Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"Test accuracy: 88.1%\nTest accuracy: 88.1%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see the range of 1000 to 1000000 for max_iter did not change the models accuracy","metadata":{}},{"cell_type":"markdown","source":"Can reducing the number of features improve accuracy?","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import RFE, SelectKBest, chi2\nfrom sklearn.model_selection import train_test_split\n\n# Using SelectKBest\nkbest_selector = SelectKBest(chi2, k=30)\nX_kbest = kbest_selector.fit_transform(X, y)\n\n\n# Split the data into training and testing sets\nOP_X_train, OP_X_test, OP_y_train, OP_y_test = train_test_split(X_kbest, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:50.669068Z","iopub.execute_input":"2023-04-27T21:21:50.670181Z","iopub.status.idle":"2023-04-27T21:21:50.742742Z","shell.execute_reply.started":"2023-04-27T21:21:50.670127Z","shell.execute_reply":"2023-04-27T21:21:50.741458Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n#help(Perceptron)\noptimized_perceptron = Perceptron(alpha=0.01, max_iter=10000, penalty= 'none')\n#help(perceptron.fit)\noptimized_perceptron.fit(OP_X_train, OP_y_train)\noptimize_accuracy = optimized_perceptron.score(OP_X_test, OP_y_test)\nprint(f'Test accuracy: {optimize_accuracy  * 100:.1f}%')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:50.744217Z","iopub.execute_input":"2023-04-27T21:21:50.744570Z","iopub.status.idle":"2023-04-27T21:21:50.757582Z","shell.execute_reply.started":"2023-04-27T21:21:50.744535Z","shell.execute_reply":"2023-04-27T21:21:50.755735Z"},"trusted":true},"execution_count":68,"outputs":[{"name":"stdout","text":"Test accuracy: 83.3%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Reducing the number of features to the 30 most imprortant improved the accuracy by only a couple precent.","metadata":{}},{"cell_type":"markdown","source":"Could changing the trainging and test split using cross validation work to improve accuracy?","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n# Instantiate the perceptron model\nperceptron = Perceptron(alpha=0.1, max_iter=100000)\n\n# Perform 5-fold cross-validation\ncv_scores = cross_val_score(perceptron, X, y, cv=10)\n\n# Print cross-validation scores and average\nprint(f\"Cross-validation scores: {cv_scores}\")\nprint(f\"Average cross-validation score: {cv_scores.mean() * 100:.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:21:50.759299Z","iopub.execute_input":"2023-04-27T21:21:50.760126Z","iopub.status.idle":"2023-04-27T21:21:50.837516Z","shell.execute_reply.started":"2023-04-27T21:21:50.760085Z","shell.execute_reply":"2023-04-27T21:21:50.834371Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Cross-validation scores: [0.23809524 0.76190476 0.42857143 0.47619048 0.76190476 0.76190476\n 0.61904762 0.85714286 0.55       0.65      ]\nAverage cross-validation score: 61.0%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"No this decreased accuracy.  ","metadata":{}},{"cell_type":"markdown","source":"Give us a quick recap of what you've done here in **Part 2**. Mention _three_ things that were most notable in this process, whether it's related to exploration, preprocessing, configuring, training, or evaluating. If you put in the effort to try to improve the perceptron, describe what you did and what let you to try what you did, and describe the results. Conclude with some questions about the problem or what you might do next to increase the performance of your classifier.\n\n**In Part 2 we took a data set with 59 features and one predictor outcome rock or mine, after exploring the data set in minor preprocessing we split the data into test and training sets and then configured, trained, and tested our perceptron resulting in 88% accuracy and showed its ability to make classifications based off  features from the dataset. Find Tuneing the methods parameters did not improve accuracy given the linear model selected.**\n\n**The first notable thing to me is how you can implement a seemingly complex model with so few lines of code given the availability of packages. That the functions in these packages can complete what would seemingly be a daunting programming challenge to a novice and simply a few lines of code when calling them. The other notable thing is how well the linear classifier worked resulting in a 88% accuracy on what looked like a fairly complex data set with 59 features I'd imagine this is a simple ML model and I'm impressed at this level of accuracy. Finally it was notable to me that I couldn't really improve the accuracy of the base configuration which in hindsight makes sense given it's a linear model but was surprised to see that the default settings essentially was trying to fight through hyper parameterization.**\n","metadata":{}},{"cell_type":"code","source":"#help(perceptron)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:30:14.979914Z","iopub.execute_input":"2023-04-27T21:30:14.980436Z","iopub.status.idle":"2023-04-27T21:30:14.985643Z","shell.execute_reply.started":"2023-04-27T21:30:14.980379Z","shell.execute_reply":"2023-04-27T21:30:14.984643Z"},"trusted":true},"execution_count":77,"outputs":[]}]}